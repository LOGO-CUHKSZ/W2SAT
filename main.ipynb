{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "\n",
    "def read_sat(sat_path):\n",
    "    with open(sat_path) as f:\n",
    "        sat_lines = f.readlines()\n",
    "        header = sat_lines[0]\n",
    "        header_info = header.replace(\"\\n\", \"\").split(\" \")\n",
    "        num_vars = int(header_info[-2])\n",
    "        num_clauses = int(header_info[-1])\n",
    "\n",
    "        sat = [[int(x) for x in line.replace(' 0\\n', '').split(' ')]\n",
    "               for line in sat_lines[1:]]\n",
    "\n",
    "        return sat, num_vars, num_clauses\n",
    "\n",
    "\n",
    "def sat_to_lig_adjacency_matrix(sat, num_vars):\n",
    "    get_literal_idx = lambda x: 2 * x - 2 if x > 0 else 2 * abs(x) - 1\n",
    "    lig_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "    lig_weighted_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "\n",
    "    for clause in sat:\n",
    "        pairs = it.combinations(clause, 2)\n",
    "#         print(f'clause: {clause}')\n",
    "        for pair in pairs:\n",
    "            x_idx = get_literal_idx(pair[0])\n",
    "            y_idx = get_literal_idx(pair[1])\n",
    "#             print(f'pair: {(x_idx, y_idx)}')\n",
    "            lig_adjacency_matrix[x_idx, y_idx] = 1\n",
    "            lig_adjacency_matrix[y_idx, x_idx] = 1\n",
    "            lig_weighted_adjacency_matrix[x_idx, y_idx] += 1\n",
    "            lig_weighted_adjacency_matrix[y_idx, x_idx] += 1    \n",
    "    return lig_adjacency_matrix, lig_weighted_adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super().__init__()\n",
    "        # GCN initialization\n",
    "        self.conv1 = GCNConv(node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        # self.conv1 = GATConv(node_features, 64, 5)\n",
    "        # self.conv2 = GATConv(64 * 5, 128)\n",
    "\n",
    "        # self.conv3 = GCNConv(128, 128)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = F.elu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = F.tanh(x)\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # x1^T * W * x2\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,   0,   0, ..., 181, 181, 181]), array([  2,   4,  14, ..., 135, 169, 178]))\n",
      "tensor([[  0,   0,   0,  ..., 181, 181, 181],\n",
      "        [  2,   4,  14,  ..., 135, 169, 178]])\n",
      "[6. 6. 1. ... 2. 1. 3.]\n",
      "histogram of original weight: (array([1516,  440,   80,   60,   14,   14]), array([1, 2, 3, 4, 5, 6, 7]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6. 1. ... 2. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "sat_path = './ssa2670-141.processed.cnf'\n",
    "sat_instance, num_vars, num_clauses = read_sat(sat_path)\n",
    "\n",
    "lig_adjacency_matrix, lig_weighted_adjacency_matrix = sat_to_lig_adjacency_matrix(sat_instance, num_vars)\n",
    "print(lig_adjacency_matrix.nonzero())\n",
    "\n",
    "\n",
    "# graph = nx.from_numpy_matrix(lig_adjacency_matrix)\n",
    "# edges = nx.to_edgelist(graph)\n",
    "# print(lig_adjacency_matrix.nonzero())\n",
    "\n",
    "edge_index = torch.tensor(lig_adjacency_matrix.nonzero(), dtype=torch.long)\n",
    "print(edge_index)\n",
    "edge_value = lig_weighted_adjacency_matrix[lig_adjacency_matrix.nonzero()]\n",
    "print(edge_value)\n",
    "print(f'histogram of original weight: {np.histogram(edge_value, bins=[1, 2, 3, 4, 5, 6, 7])}')\n",
    "plt.hist(edge_value, bins=[1, 2, 3, 4, 5, 6, 7])\n",
    "plt.show()\n",
    "# max_edge_value = max(edge_value)\n",
    "# norm_edge_value = edge_value/max_edge_value\n",
    "norm_edge_value = edge_value\n",
    "print(norm_edge_value)\n",
    "\n",
    "embeddings = torch.load('./embeddings.pt')\n",
    "embeddings.requires_grad = False\n",
    "# print(embeddings)\n",
    "x = embeddings\n",
    "data = Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.1641110181808472\n",
      "epoch: 1, loss: 1.4175746440887451\n",
      "epoch: 2, loss: 0.7755216956138611\n",
      "epoch: 3, loss: 2.072573184967041\n",
      "epoch: 4, loss: 0.7336053252220154\n",
      "epoch: 5, loss: 0.8084513545036316\n",
      "epoch: 6, loss: 1.0635331869125366\n",
      "epoch: 7, loss: 1.1253913640975952\n",
      "epoch: 8, loss: 1.0059610605239868\n",
      "epoch: 9, loss: 0.7969239950180054\n",
      "epoch: 10, loss: 0.686897873878479\n",
      "epoch: 11, loss: 0.8059244751930237\n",
      "epoch: 12, loss: 0.8264654874801636\n",
      "epoch: 13, loss: 0.6877186298370361\n",
      "epoch: 14, loss: 0.609920859336853\n",
      "epoch: 15, loss: 0.629022479057312\n",
      "epoch: 16, loss: 0.6754872798919678\n",
      "epoch: 17, loss: 0.6977923512458801\n",
      "epoch: 18, loss: 0.6801174879074097\n",
      "epoch: 19, loss: 0.6346316933631897\n",
      "epoch: 20, loss: 0.5933061242103577\n",
      "epoch: 21, loss: 0.5881275534629822\n",
      "epoch: 22, loss: 0.6123658418655396\n",
      "epoch: 23, loss: 0.6167895197868347\n",
      "epoch: 24, loss: 0.5849300026893616\n",
      "epoch: 25, loss: 0.5537311434745789\n",
      "epoch: 26, loss: 0.5475904941558838\n",
      "epoch: 27, loss: 0.5561403036117554\n",
      "epoch: 28, loss: 0.560667872428894\n",
      "epoch: 29, loss: 0.554793119430542\n",
      "epoch: 30, loss: 0.5452896952629089\n",
      "epoch: 31, loss: 0.5413373112678528\n",
      "epoch: 32, loss: 0.5422452092170715\n",
      "epoch: 33, loss: 0.5388342142105103\n",
      "epoch: 34, loss: 0.5282092094421387\n",
      "epoch: 35, loss: 0.5174251794815063\n",
      "epoch: 36, loss: 0.5123142600059509\n",
      "epoch: 37, loss: 0.5113287568092346\n",
      "epoch: 38, loss: 0.509600043296814\n",
      "epoch: 39, loss: 0.5051125288009644\n",
      "epoch: 40, loss: 0.5003016591072083\n",
      "epoch: 41, loss: 0.498439222574234\n",
      "epoch: 42, loss: 0.49840596318244934\n",
      "epoch: 43, loss: 0.4954877197742462\n",
      "epoch: 44, loss: 0.48835423588752747\n",
      "epoch: 45, loss: 0.4810325503349304\n",
      "epoch: 46, loss: 0.476516455411911\n",
      "epoch: 47, loss: 0.47382083535194397\n",
      "epoch: 48, loss: 0.47097209095954895\n",
      "epoch: 49, loss: 0.46775221824645996\n",
      "epoch: 50, loss: 0.4649500250816345\n",
      "epoch: 51, loss: 0.46238768100738525\n",
      "epoch: 52, loss: 0.45892953872680664\n",
      "epoch: 53, loss: 0.4544328451156616\n",
      "epoch: 54, loss: 0.45022743940353394\n",
      "epoch: 55, loss: 0.44723719358444214\n",
      "epoch: 56, loss: 0.44484028220176697\n",
      "epoch: 57, loss: 0.44212767481803894\n",
      "epoch: 58, loss: 0.4391126334667206\n",
      "epoch: 59, loss: 0.43645188212394714\n",
      "epoch: 60, loss: 0.43432602286338806\n",
      "epoch: 61, loss: 0.4321270287036896\n",
      "epoch: 62, loss: 0.42947888374328613\n",
      "epoch: 63, loss: 0.426756352186203\n",
      "epoch: 64, loss: 0.4243919253349304\n",
      "epoch: 65, loss: 0.42234310507774353\n",
      "epoch: 66, loss: 0.42037200927734375\n",
      "epoch: 67, loss: 0.4184548258781433\n",
      "epoch: 68, loss: 0.41657406091690063\n",
      "epoch: 69, loss: 0.414610892534256\n",
      "epoch: 70, loss: 0.4125683605670929\n",
      "epoch: 71, loss: 0.41055577993392944\n",
      "epoch: 72, loss: 0.4085884094238281\n",
      "epoch: 73, loss: 0.40659284591674805\n",
      "epoch: 74, loss: 0.40459534525871277\n",
      "epoch: 75, loss: 0.40269026160240173\n",
      "epoch: 76, loss: 0.4008511006832123\n",
      "epoch: 77, loss: 0.39887872338294983\n",
      "epoch: 78, loss: 0.3967428505420685\n",
      "epoch: 79, loss: 0.3946465849876404\n",
      "epoch: 80, loss: 0.3926992416381836\n",
      "epoch: 81, loss: 0.3907700777053833\n",
      "epoch: 82, loss: 0.3887663185596466\n",
      "epoch: 83, loss: 0.3867347240447998\n",
      "epoch: 84, loss: 0.3847161829471588\n",
      "epoch: 85, loss: 0.38268256187438965\n",
      "epoch: 86, loss: 0.38062751293182373\n",
      "epoch: 87, loss: 0.37859684228897095\n",
      "epoch: 88, loss: 0.376593679189682\n",
      "epoch: 89, loss: 0.37455326318740845\n",
      "epoch: 90, loss: 0.3725149929523468\n",
      "epoch: 91, loss: 0.3704945743083954\n",
      "epoch: 92, loss: 0.3684348165988922\n",
      "epoch: 93, loss: 0.36632153391838074\n",
      "epoch: 94, loss: 0.3642183542251587\n",
      "epoch: 95, loss: 0.362148255109787\n",
      "epoch: 96, loss: 0.3600552976131439\n",
      "epoch: 97, loss: 0.3579447567462921\n",
      "epoch: 98, loss: 0.3558215796947479\n",
      "epoch: 99, loss: 0.3536776900291443\n",
      "epoch: 100, loss: 0.351531982421875\n",
      "epoch: 101, loss: 0.3493969142436981\n",
      "epoch: 102, loss: 0.3472713530063629\n",
      "epoch: 103, loss: 0.3451429605484009\n",
      "epoch: 104, loss: 0.3430194854736328\n",
      "epoch: 105, loss: 0.34089112281799316\n",
      "epoch: 106, loss: 0.33877214789390564\n",
      "epoch: 107, loss: 0.3366725444793701\n",
      "epoch: 108, loss: 0.3345943093299866\n",
      "epoch: 109, loss: 0.33252811431884766\n",
      "epoch: 110, loss: 0.33047622442245483\n",
      "epoch: 111, loss: 0.3284277021884918\n",
      "epoch: 112, loss: 0.32637500762939453\n",
      "epoch: 113, loss: 0.3243280053138733\n",
      "epoch: 114, loss: 0.32230377197265625\n",
      "epoch: 115, loss: 0.32028383016586304\n",
      "epoch: 116, loss: 0.3182556629180908\n",
      "epoch: 117, loss: 0.3162093758583069\n",
      "epoch: 118, loss: 0.3141864836215973\n",
      "epoch: 119, loss: 0.3121838867664337\n",
      "epoch: 120, loss: 0.31017670035362244\n",
      "epoch: 121, loss: 0.30817389488220215\n",
      "epoch: 122, loss: 0.3061719834804535\n",
      "epoch: 123, loss: 0.3041687607765198\n",
      "epoch: 124, loss: 0.3021804094314575\n",
      "epoch: 125, loss: 0.3001956641674042\n",
      "epoch: 126, loss: 0.29821091890335083\n",
      "epoch: 127, loss: 0.2962130606174469\n",
      "epoch: 128, loss: 0.29421791434288025\n",
      "epoch: 129, loss: 0.2922118604183197\n",
      "epoch: 130, loss: 0.29021334648132324\n",
      "epoch: 131, loss: 0.28821447491645813\n",
      "epoch: 132, loss: 0.28621575236320496\n",
      "epoch: 133, loss: 0.2842148542404175\n",
      "epoch: 134, loss: 0.28221404552459717\n",
      "epoch: 135, loss: 0.2802220284938812\n",
      "epoch: 136, loss: 0.2782294452190399\n",
      "epoch: 137, loss: 0.27625545859336853\n",
      "epoch: 138, loss: 0.27428606152534485\n",
      "epoch: 139, loss: 0.2723280191421509\n",
      "epoch: 140, loss: 0.27038130164146423\n",
      "epoch: 141, loss: 0.2684404253959656\n",
      "epoch: 142, loss: 0.2664913535118103\n",
      "epoch: 143, loss: 0.26455676555633545\n",
      "epoch: 144, loss: 0.26262450218200684\n",
      "epoch: 145, loss: 0.260695219039917\n",
      "epoch: 146, loss: 0.25876089930534363\n",
      "epoch: 147, loss: 0.2568275034427643\n",
      "epoch: 148, loss: 0.25491148233413696\n",
      "epoch: 149, loss: 0.2530139982700348\n",
      "epoch: 150, loss: 0.2511301338672638\n",
      "epoch: 151, loss: 0.24925853312015533\n",
      "epoch: 152, loss: 0.24739430844783783\n",
      "epoch: 153, loss: 0.2455390840768814\n",
      "epoch: 154, loss: 0.24369125068187714\n",
      "epoch: 155, loss: 0.24184836447238922\n",
      "epoch: 156, loss: 0.240021213889122\n",
      "epoch: 157, loss: 0.23820985853672028\n",
      "epoch: 158, loss: 0.2364014834165573\n",
      "epoch: 159, loss: 0.23458901047706604\n",
      "epoch: 160, loss: 0.23277075588703156\n",
      "epoch: 161, loss: 0.23096328973770142\n",
      "epoch: 162, loss: 0.22916166484355927\n",
      "epoch: 163, loss: 0.22737322747707367\n",
      "epoch: 164, loss: 0.22558188438415527\n",
      "epoch: 165, loss: 0.2238050252199173\n",
      "epoch: 166, loss: 0.2220277339220047\n",
      "epoch: 167, loss: 0.22024931013584137\n",
      "epoch: 168, loss: 0.21847468614578247\n",
      "epoch: 169, loss: 0.21670155227184296\n",
      "epoch: 170, loss: 0.21494126319885254\n",
      "epoch: 171, loss: 0.2131980061531067\n",
      "epoch: 172, loss: 0.21146751940250397\n",
      "epoch: 173, loss: 0.20974871516227722\n",
      "epoch: 174, loss: 0.20803983509540558\n",
      "epoch: 175, loss: 0.20634250342845917\n",
      "epoch: 176, loss: 0.2046622782945633\n",
      "epoch: 177, loss: 0.2030009627342224\n",
      "epoch: 178, loss: 0.20135745406150818\n",
      "epoch: 179, loss: 0.19973547756671906\n",
      "epoch: 180, loss: 0.19814357161521912\n",
      "epoch: 181, loss: 0.19658799469470978\n",
      "epoch: 182, loss: 0.19505874812602997\n",
      "epoch: 183, loss: 0.1935620903968811\n",
      "epoch: 184, loss: 0.1920974850654602\n",
      "epoch: 185, loss: 0.19068095088005066\n",
      "epoch: 186, loss: 0.18932946026325226\n",
      "epoch: 187, loss: 0.18812748789787292\n",
      "epoch: 188, loss: 0.18712085485458374\n",
      "epoch: 189, loss: 0.18637070059776306\n",
      "epoch: 190, loss: 0.18586020171642303\n",
      "epoch: 191, loss: 0.1844552755355835\n",
      "epoch: 192, loss: 0.18291883170604706\n",
      "epoch: 193, loss: 0.18068791925907135\n",
      "epoch: 194, loss: 0.17900973558425903\n",
      "epoch: 195, loss: 0.1779453456401825\n",
      "epoch: 196, loss: 0.17732058465480804\n",
      "epoch: 197, loss: 0.17695392668247223\n",
      "epoch: 198, loss: 0.1761590987443924\n",
      "epoch: 199, loss: 0.17541354894638062\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model = GCN(50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    src, dst = edge_index\n",
    "    score = (out[src] * out[dst]).sum(dim=-1)\n",
    "    # score = torch.sigmoid(score)\n",
    "    loss = F.mse_loss(score, torch.tensor(norm_edge_value, dtype=torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch: {epoch}, loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# TBD\n",
    "# 1. product sum \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_edge_value: [6. 6. 1. ... 2. 1. 3.]\n",
      "score: tensor([4.8779, 6.0709, 0.7641,  ..., 2.1305, 2.0319, 3.2457])\n",
      "min score: 0.3705979585647583\n",
      "max score: 6.070923328399658\n"
     ]
    }
   ],
   "source": [
    "out = model(data)\n",
    "src, dst = edge_index\n",
    "score = (out[src] * out[dst]).sum(dim=-1)\n",
    "# print(min(score))\n",
    "# score = torch.sigmoid(score)\n",
    "print(f\"norm_edge_value: {norm_edge_value}\")\n",
    "print(f\"score: {score.detach()}\")\n",
    "print(f\"min score: {min(score)}\")\n",
    "print(f\"max score: {max(score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<182x182 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2124 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig_adjacency_matrix\n",
    "sparse_matrix = sparse.csr_matrix(lig_adjacency_matrix)\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  10/200 Loss: 3.92775 Edge-Overlap: 0.388 Total-Time: 0\n",
      "Step:  20/200 Loss: 3.35798 Edge-Overlap: 0.570 Total-Time: 0\n"
     ]
    }
   ],
   "source": [
    "# CELL\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import torch\n",
    "\n",
    "from cell.utils import link_prediction_performance\n",
    "from cell.cell import Cell, EdgeOverlapCriterion, LinkPredictionCriterion\n",
    "from cell.graph_statistics import compute_graph_statistics\n",
    "\n",
    "cell_model = Cell(A=sparse_matrix,\n",
    "             H=9,\n",
    "             callbacks=[EdgeOverlapCriterion(invoke_every=10, edge_overlap_limit=.5)])\n",
    "\n",
    "\n",
    "cell_model.train(steps=200,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_args={'lr': 0.1,\n",
    "                            'weight_decay': 1e-7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(array([  0,   0,   0, ..., 181, 181, 181]), array([  2,  14,  15, ..., 134, 135, 149]))\n"
     ]
    }
   ],
   "source": [
    "generated_graph = cell_model.sample_graph()\n",
    "print(generated_graph.A)\n",
    "print(generated_graph.A.nonzero())\n",
    "# compute_graph_statistics(generated_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2124])\n",
      "tensor([[ 0.1458, -0.0650, -0.2092,  ...,  0.3151,  0.2128, -0.0984],\n",
      "        [-0.0693, -0.1420, -0.3167,  ...,  0.2207,  0.0885, -0.0737],\n",
      "        [ 0.0043, -0.1312, -0.1235,  ...,  0.1458,  0.1412, -0.0531],\n",
      "        ...,\n",
      "        [-0.1151,  0.0292,  0.0620,  ..., -0.1073, -0.0698, -0.0251],\n",
      "        [-0.1774,  0.0396,  0.1497,  ..., -0.3238, -0.2087,  0.0173],\n",
      "        [-0.1513,  0.0180,  0.2388,  ...,  0.1160,  0.1926, -0.0532]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 2.4964,  0.8891, -0.7602,  ...,  2.4207,  3.3430,  0.5872],\n",
      "       grad_fn=<SumBackward1>)\n",
      "2124\n",
      "histogram of inference weight: (array([1604,  424,   84,    6,    6,    0]), array([1, 2, 3, 4, 5, 6, 7]))\n"
     ]
    }
   ],
   "source": [
    "graph_prime = generated_graph.A\n",
    "edge_index_prime = torch.tensor(graph_prime.nonzero(), dtype=torch.long)\n",
    "print(edge_index_prime.size())\n",
    "data_prime = Data(x=x, edge_index = edge_index_prime)\n",
    "out = model(data_prime)\n",
    "print(out)\n",
    "src, dst = edge_index_prime\n",
    "score = (out[src] * out[dst]).sum(dim=-1)\n",
    "print(score)\n",
    "print(len(score))\n",
    "weight = score.detach().numpy()\n",
    "weight[weight <= 1] = 1\n",
    "weight = np.rint(weight).astype(int)\n",
    "print(f'histogram of inference weight: {np.histogram(weight, bins=[1, 2, 3, 4, 5, 6, 7])}')\n",
    "# plt.hist(weight, bins=[1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_graph_prime = graph_prime\n",
    "weighted_graph_prime[weighted_graph_prime.nonzero()] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clique_candidates(lig_adjacency_matrix, k):\n",
    "    graph = nx.from_numpy_matrix(lig_adjacency_matrix)\n",
    "    cliques = nx.enumerate_all_cliques(graph)\n",
    "    clique_candidates = []\n",
    "    for clique in cliques:\n",
    "        if len(clique) <= k:\n",
    "            if len(clique) > 1:\n",
    "                clique_candidates.append(clique)\n",
    "        else:\n",
    "            break\n",
    "    return clique_candidates\n",
    "\n",
    "\n",
    "def cliques_to_weighted_adjacency_matrix(cliques, num_vars):\n",
    "    weighted_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "    for clique in cliques:\n",
    "        pairs = it.combinations(clique, 2)\n",
    "        for pair in pairs:\n",
    "            x_idx = pair[0]\n",
    "            y_idx = pair[1]\n",
    "\n",
    "            weighted_adjacency_matrix[x_idx, y_idx] += 1\n",
    "            weighted_adjacency_matrix[y_idx, x_idx] += 1\n",
    "\n",
    "    return weighted_adjacency_matrix\n",
    "\n",
    "\n",
    "def objective(lig_weighted_adjacency_matrix, lig_weighted_adjacency_matrix_p):\n",
    "    return ((lig_weighted_adjacency_matrix - lig_weighted_adjacency_matrix_p)**2).mean()\n",
    "\n",
    "\n",
    "def greedy_clique_edge_cover(lig_weighted_adjacency_matrix, clique_candidates, cliques_quota, num_vars):\n",
    "    current_clique_idxs = []\n",
    "    current_cliques = [clique_candidates[idx] for idx in current_clique_idxs]\n",
    "    current_weighted_adjacency_matrix = cliques_to_weighted_adjacency_matrix(current_cliques, num_vars)\n",
    "    print(current_weighted_adjacency_matrix.shape)\n",
    "    current_objective = objective(lig_weighted_adjacency_matrix, current_weighted_adjacency_matrix)\n",
    "\n",
    "    clique_candidates_idx = set(range(len(clique_candidates)))\n",
    "    min_clique_idx = []\n",
    "    for i in range(cliques_quota):\n",
    "        min_objective = 10\n",
    "        for idx in clique_candidates_idx - set(current_clique_idxs):\n",
    "            temp_clique_idx = current_clique_idxs + [idx]\n",
    "            temp_cliques = [clique_candidates[idx] for idx in temp_clique_idx]\n",
    "            temp_weighted_adjacency_matrix = cliques_to_weighted_adjacency_matrix(temp_cliques, num_vars)\n",
    "            temp_objective = objective(lig_weighted_adjacency_matrix, temp_weighted_adjacency_matrix)\n",
    "            if temp_objective < min_objective:\n",
    "                min_objective = temp_objective\n",
    "                min_clique_idx = temp_clique_idx\n",
    "            \n",
    "        current_clique_idxs = min_clique_idx\n",
    "        current_objective = min_objective\n",
    "\n",
    "        print(f'iteration: {i}, current_objective: {current_objective}')\n",
    "\n",
    "\n",
    "    return current_clique_idxs, current_objective\n",
    "\n",
    "def cliques_to_sat(cliques):\n",
    "    sat = []\n",
    "\n",
    "    for clique in cliques:\n",
    "        clause = [int((x + 2)/2) if x % 2 == 0 else int(-(x + 1)/2) for x in clique]\n",
    "        sat.append(clause)\n",
    "    \n",
    "    return sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182, 182)\n",
      "(182, 182)\n",
      "iteration: 0, current_objective: 0.12649438473614297\n",
      "iteration: 1, current_objective: 0.12353580485448618\n",
      "iteration: 2, current_objective: 0.1213621543291873\n",
      "iteration: 3, current_objective: 0.11967153725395484\n",
      "iteration: 4, current_objective: 0.11798092017872237\n",
      "iteration: 5, current_objective: 0.11653181982852312\n",
      "iteration: 6, current_objective: 0.11508271947832388\n",
      "iteration: 7, current_objective: 0.11363361912812463\n",
      "iteration: 8, current_objective: 0.11230527714044197\n",
      "iteration: 9, current_objective: 0.11097693515275933\n",
      "iteration: 10, current_objective: 0.10964859316507668\n",
      "iteration: 11, current_objective: 0.10832025117739404\n",
      "iteration: 12, current_objective: 0.10699190918971138\n",
      "iteration: 13, current_objective: 0.10578432556454534\n",
      "iteration: 14, current_objective: 0.10463712112063761\n",
      "iteration: 15, current_objective: 0.10355029585798817\n",
      "iteration: 16, current_objective: 0.10246347059533872\n",
      "iteration: 17, current_objective: 0.1013766453326893\n",
      "iteration: 18, current_objective: 0.10041057843255645\n",
      "iteration: 19, current_objective: 0.09944451153242362\n",
      "iteration: 20, current_objective: 0.09847844463229079\n",
      "iteration: 21, current_objective: 0.09751237773215796\n",
      "iteration: 22, current_objective: 0.09654631083202511\n",
      "iteration: 23, current_objective: 0.09558024393189228\n",
      "iteration: 24, current_objective: 0.09461417703175945\n",
      "iteration: 25, current_objective: 0.09370848931288492\n",
      "iteration: 26, current_objective: 0.09280280159401039\n",
      "iteration: 27, current_objective: 0.09195749305639415\n",
      "iteration: 28, current_objective: 0.09111218451877792\n",
      "iteration: 29, current_objective: 0.09026687598116169\n",
      "iteration: 30, current_objective: 0.08942156744354547\n",
      "iteration: 31, current_objective: 0.08863663808718754\n",
      "iteration: 32, current_objective: 0.08791208791208792\n",
      "iteration: 33, current_objective: 0.08718753773698829\n",
      "iteration: 34, current_objective: 0.08646298756188867\n",
      "iteration: 35, current_objective: 0.08573843738678903\n",
      "iteration: 36, current_objective: 0.08501388721168941\n",
      "iteration: 37, current_objective: 0.08434971621784809\n",
      "iteration: 38, current_objective: 0.08374592440526507\n",
      "iteration: 39, current_objective: 0.08314213259268205\n",
      "iteration: 40, current_objective: 0.08253834078009903\n",
      "iteration: 41, current_objective: 0.081934548967516\n",
      "iteration: 42, current_objective: 0.08133075715493297\n",
      "iteration: 43, current_objective: 0.08072696534234995\n",
      "iteration: 44, current_objective: 0.08012317352976693\n",
      "iteration: 45, current_objective: 0.07951938171718391\n",
      "iteration: 46, current_objective: 0.07891558990460089\n",
      "iteration: 47, current_objective: 0.07831179809201787\n",
      "iteration: 48, current_objective: 0.07770800627943485\n",
      "iteration: 49, current_objective: 0.07716459364811014\n",
      "iteration: 50, current_objective: 0.07662118101678542\n",
      "iteration: 51, current_objective: 0.0760777683854607\n",
      "iteration: 52, current_objective: 0.07553435575413597\n",
      "iteration: 53, current_objective: 0.07499094312281125\n",
      "iteration: 54, current_objective: 0.07444753049148653\n",
      "iteration: 55, current_objective: 0.07390411786016182\n",
      "iteration: 56, current_objective: 0.0733607052288371\n",
      "iteration: 57, current_objective: 0.07281729259751238\n",
      "iteration: 58, current_objective: 0.07227387996618766\n",
      "iteration: 59, current_objective: 0.07173046733486294\n",
      "iteration: 60, current_objective: 0.07124743388479653\n",
      "iteration: 61, current_objective: 0.0707644004347301\n",
      "iteration: 62, current_objective: 0.07028136698466368\n",
      "iteration: 63, current_objective: 0.06979833353459727\n",
      "iteration: 64, current_objective: 0.06931530008453085\n",
      "iteration: 65, current_objective: 0.06883226663446444\n",
      "iteration: 66, current_objective: 0.06834923318439802\n",
      "iteration: 67, current_objective: 0.0678661997343316\n",
      "iteration: 68, current_objective: 0.06738316628426519\n",
      "iteration: 69, current_objective: 0.06696051201545707\n",
      "iteration: 70, current_objective: 0.06653785774664896\n",
      "iteration: 71, current_objective: 0.06611520347784083\n",
      "iteration: 72, current_objective: 0.06569254920903272\n",
      "iteration: 73, current_objective: 0.06526989494022462\n",
      "iteration: 74, current_objective: 0.06484724067141649\n",
      "iteration: 75, current_objective: 0.06442458640260838\n",
      "iteration: 76, current_objective: 0.06400193213380026\n",
      "iteration: 77, current_objective: 0.06357927786499215\n",
      "iteration: 78, current_objective: 0.06315662359618404\n",
      "iteration: 79, current_objective: 0.06273396932737592\n",
      "iteration: 80, current_objective: 0.0623113150585678\n",
      "iteration: 81, current_objective: 0.06188866078975969\n",
      "iteration: 82, current_objective: 0.061526385702209876\n",
      "iteration: 83, current_objective: 0.06116411061466007\n",
      "iteration: 84, current_objective: 0.06080183552711025\n",
      "iteration: 85, current_objective: 0.06043956043956044\n",
      "iteration: 86, current_objective: 0.06007728535201063\n",
      "iteration: 87, current_objective: 0.05971501026446081\n",
      "iteration: 88, current_objective: 0.059352735176911\n",
      "iteration: 89, current_objective: 0.05899046008936119\n",
      "iteration: 90, current_objective: 0.05862818500181138\n",
      "iteration: 91, current_objective: 0.05826590991426156\n",
      "iteration: 92, current_objective: 0.05790363482671175\n",
      "iteration: 93, current_objective: 0.05754135973916194\n",
      "iteration: 94, current_objective: 0.05717908465161212\n",
      "iteration: 95, current_objective: 0.056816809564062314\n",
      "iteration: 96, current_objective: 0.0565149136577708\n",
      "iteration: 97, current_objective: 0.05621301775147929\n",
      "iteration: 98, current_objective: 0.05591112184518778\n",
      "iteration: 99, current_objective: 0.05560922593889627\n",
      "iteration: 100, current_objective: 0.05530733003260476\n",
      "iteration: 101, current_objective: 0.05500543412631325\n",
      "iteration: 102, current_objective: 0.05470353822002174\n",
      "iteration: 103, current_objective: 0.05440164231373022\n",
      "iteration: 104, current_objective: 0.05409974640743871\n",
      "iteration: 105, current_objective: 0.0537978505011472\n",
      "iteration: 106, current_objective: 0.05349595459485569\n",
      "iteration: 107, current_objective: 0.05319405868856418\n",
      "iteration: 108, current_objective: 0.05289216278227267\n",
      "iteration: 109, current_objective: 0.05259026687598116\n",
      "iteration: 110, current_objective: 0.05228837096968965\n",
      "iteration: 111, current_objective: 0.05198647506339814\n",
      "iteration: 112, current_objective: 0.05168457915710663\n",
      "iteration: 113, current_objective: 0.05138268325081512\n",
      "iteration: 114, current_objective: 0.05108078734452361\n",
      "iteration: 115, current_objective: 0.050778891438232096\n",
      "iteration: 116, current_objective: 0.050476995531940586\n",
      "iteration: 117, current_objective: 0.050175099625649075\n",
      "iteration: 118, current_objective: 0.049873203719357564\n",
      "iteration: 119, current_objective: 0.049571307813066054\n",
      "iteration: 120, current_objective: 0.04926941190677454\n",
      "iteration: 121, current_objective: 0.04896751600048303\n",
      "iteration: 122, current_objective: 0.04872599927544982\n",
      "iteration: 123, current_objective: 0.04848448255041662\n",
      "iteration: 124, current_objective: 0.048242965825383406\n",
      "iteration: 125, current_objective: 0.0480014491003502\n",
      "iteration: 126, current_objective: 0.04775993237531699\n",
      "iteration: 127, current_objective: 0.04751841565028378\n",
      "iteration: 128, current_objective: 0.047276898925250575\n",
      "iteration: 129, current_objective: 0.047035382200217364\n",
      "iteration: 130, current_objective: 0.04679386547518416\n",
      "iteration: 131, current_objective: 0.04661272793140925\n",
      "iteration: 132, current_objective: 0.04643159038763434\n",
      "iteration: 133, current_objective: 0.04625045284385944\n",
      "iteration: 134, current_objective: 0.04606931530008453\n",
      "iteration: 135, current_objective: 0.04588817775630962\n",
      "iteration: 136, current_objective: 0.045707040212534716\n",
      "iteration: 137, current_objective: 0.04552590266875981\n",
      "iteration: 138, current_objective: 0.045344765124984907\n",
      "iteration: 139, current_objective: 0.04516362758121\n",
      "iteration: 140, current_objective: 0.04498249003743509\n",
      "iteration: 141, current_objective: 0.044801352493660185\n",
      "iteration: 142, current_objective: 0.04462021494988528\n",
      "iteration: 143, current_objective: 0.044439077406110375\n",
      "iteration: 144, current_objective: 0.04425793986233547\n",
      "iteration: 145, current_objective: 0.04407680231856056\n",
      "iteration: 146, current_objective: 0.04389566477478565\n",
      "iteration: 147, current_objective: 0.04371452723101075\n",
      "iteration: 148, current_objective: 0.04353338968723584\n",
      "iteration: 149, current_objective: 0.04335225214346093\n",
      "iteration: 150, current_objective: 0.04317111459968603\n",
      "iteration: 151, current_objective: 0.04298997705591112\n",
      "iteration: 152, current_objective: 0.04280883951213622\n",
      "iteration: 153, current_objective: 0.04262770196836131\n",
      "iteration: 154, current_objective: 0.0424465644245864\n",
      "iteration: 155, current_objective: 0.042265426880811495\n",
      "iteration: 156, current_objective: 0.04208428933703659\n",
      "iteration: 157, current_objective: 0.041903151793261685\n",
      "iteration: 158, current_objective: 0.04172201424948678\n",
      "iteration: 159, current_objective: 0.04154087670571187\n",
      "iteration: 160, current_objective: 0.04135973916193696\n",
      "iteration: 161, current_objective: 0.04117860161816206\n",
      "iteration: 162, current_objective: 0.040997464074387154\n",
      "iteration: 163, current_objective: 0.04081632653061224\n",
      "iteration: 164, current_objective: 0.04063518898683734\n",
      "iteration: 165, current_objective: 0.04045405144306243\n",
      "iteration: 166, current_objective: 0.04027291389928753\n",
      "iteration: 167, current_objective: 0.04009177635551262\n",
      "iteration: 168, current_objective: 0.03991063881173771\n",
      "iteration: 169, current_objective: 0.039729501267962805\n",
      "iteration: 170, current_objective: 0.0395483637241879\n",
      "iteration: 171, current_objective: 0.039367226180412995\n",
      "iteration: 172, current_objective: 0.03918608863663809\n",
      "iteration: 173, current_objective: 0.03900495109286318\n",
      "iteration: 174, current_objective: 0.038823813549088274\n",
      "iteration: 175, current_objective: 0.03864267600531337\n",
      "iteration: 176, current_objective: 0.038461538461538464\n",
      "iteration: 177, current_objective: 0.03828040091776355\n",
      "iteration: 178, current_objective: 0.03809926337398865\n",
      "iteration: 179, current_objective: 0.03791812583021374\n",
      "iteration: 180, current_objective: 0.03773698828643884\n",
      "iteration: 181, current_objective: 0.03755585074266393\n",
      "iteration: 182, current_objective: 0.03737471319888902\n",
      "iteration: 183, current_objective: 0.037193575655114115\n",
      "iteration: 184, current_objective: 0.03701243811133921\n",
      "iteration: 185, current_objective: 0.036831300567564305\n",
      "iteration: 186, current_objective: 0.0366501630237894\n",
      "iteration: 187, current_objective: 0.03646902548001449\n",
      "iteration: 188, current_objective: 0.036287887936239584\n",
      "iteration: 189, current_objective: 0.03610675039246468\n",
      "iteration: 190, current_objective: 0.035925612848689774\n",
      "iteration: 191, current_objective: 0.03574447530491486\n",
      "iteration: 192, current_objective: 0.03556333776113996\n",
      "iteration: 193, current_objective: 0.03538220021736505\n",
      "iteration: 194, current_objective: 0.03520106267359015\n",
      "iteration: 195, current_objective: 0.03501992512981524\n",
      "iteration: 196, current_objective: 0.03483878758604033\n",
      "iteration: 197, current_objective: 0.034657650042265425\n",
      "iteration: 198, current_objective: 0.03447651249849052\n",
      "iteration: 199, current_objective: 0.034295374954715616\n",
      "iteration: 200, current_objective: 0.03411423741094071\n",
      "iteration: 201, current_objective: 0.0339330998671658\n",
      "iteration: 202, current_objective: 0.033751962323390894\n",
      "iteration: 203, current_objective: 0.03357082477961599\n",
      "iteration: 204, current_objective: 0.033389687235841084\n",
      "iteration: 205, current_objective: 0.03320854969206617\n",
      "iteration: 206, current_objective: 0.03302741214829127\n",
      "iteration: 207, current_objective: 0.03284627460451636\n",
      "iteration: 208, current_objective: 0.03266513706074146\n",
      "iteration: 209, current_objective: 0.03248399951696655\n",
      "iteration: 210, current_objective: 0.03230286197319164\n",
      "iteration: 211, current_objective: 0.032121724429416736\n",
      "iteration: 212, current_objective: 0.03194058688564183\n",
      "iteration: 213, current_objective: 0.031759449341866926\n",
      "iteration: 214, current_objective: 0.03157831179809202\n",
      "iteration: 215, current_objective: 0.03139717425431711\n",
      "iteration: 216, current_objective: 0.031216036710542204\n",
      "iteration: 217, current_objective: 0.0310348991667673\n",
      "iteration: 218, current_objective: 0.03085376162299239\n",
      "iteration: 219, current_objective: 0.030672624079217486\n",
      "iteration: 220, current_objective: 0.03049148653544258\n",
      "iteration: 221, current_objective: 0.030310348991667672\n",
      "iteration: 222, current_objective: 0.030129211447892768\n",
      "iteration: 223, current_objective: 0.02994807390411786\n",
      "iteration: 224, current_objective: 0.029766936360342954\n",
      "iteration: 225, current_objective: 0.029585798816568046\n",
      "iteration: 226, current_objective: 0.02940466127279314\n",
      "iteration: 227, current_objective: 0.029223523729018236\n",
      "iteration: 228, current_objective: 0.029042386185243328\n",
      "iteration: 229, current_objective: 0.028861248641468423\n",
      "iteration: 230, current_objective: 0.02880086946021012\n",
      "iteration: 231, current_objective: 0.028740490278951817\n",
      "iteration: 232, current_objective: 0.028680111097693514\n",
      "iteration: 233, current_objective: 0.028619731916435215\n",
      "iteration: 234, current_objective: 0.028559352735176912\n",
      "iteration: 235, current_objective: 0.02849897355391861\n",
      "iteration: 236, current_objective: 0.028438594372660306\n",
      "iteration: 237, current_objective: 0.028378215191402004\n",
      "iteration: 238, current_objective: 0.0283178360101437\n",
      "iteration: 239, current_objective: 0.0282574568288854\n",
      "iteration: 240, current_objective: 0.0281970776476271\n",
      "iteration: 241, current_objective: 0.028136698466368796\n",
      "iteration: 242, current_objective: 0.028076319285110493\n",
      "iteration: 243, current_objective: 0.02801594010385219\n",
      "iteration: 244, current_objective: 0.02795556092259389\n",
      "iteration: 245, current_objective: 0.027895181741335588\n",
      "iteration: 246, current_objective: 0.027834802560077285\n",
      "iteration: 247, current_objective: 0.027774423378818983\n",
      "iteration: 248, current_objective: 0.02771404419756068\n",
      "iteration: 249, current_objective: 0.02765366501630238\n",
      "iteration: 250, current_objective: 0.027593285835044078\n",
      "iteration: 251, current_objective: 0.027532906653785775\n",
      "iteration: 252, current_objective: 0.027472527472527472\n",
      "iteration: 253, current_objective: 0.02741214829126917\n",
      "iteration: 254, current_objective: 0.02735176911001087\n",
      "iteration: 255, current_objective: 0.027291389928752567\n",
      "iteration: 256, current_objective: 0.027231010747494264\n",
      "iteration: 257, current_objective: 0.02717063156623596\n",
      "iteration: 258, current_objective: 0.02711025238497766\n",
      "iteration: 259, current_objective: 0.027049873203719356\n",
      "iteration: 260, current_objective: 0.026989494022461057\n",
      "iteration: 261, current_objective: 0.026929114841202754\n",
      "iteration: 262, current_objective: 0.02686873565994445\n",
      "iteration: 263, current_objective: 0.026808356478686148\n",
      "iteration: 264, current_objective: 0.026747977297427845\n",
      "iteration: 265, current_objective: 0.026687598116169546\n",
      "iteration: 266, current_objective: 0.026627218934911243\n",
      "iteration: 267, current_objective: 0.02656683975365294\n",
      "iteration: 268, current_objective: 0.026506460572394638\n",
      "iteration: 269, current_objective: 0.026446081391136335\n",
      "iteration: 270, current_objective: 0.026385702209878036\n",
      "iteration: 271, current_objective: 0.026325323028619733\n",
      "iteration: 272, current_objective: 0.02626494384736143\n",
      "iteration: 273, current_objective: 0.026204564666103127\n",
      "iteration: 274, current_objective: 0.026144185484844824\n",
      "iteration: 275, current_objective: 0.026083806303586525\n",
      "iteration: 276, current_objective: 0.026023427122328222\n",
      "iteration: 277, current_objective: 0.02596304794106992\n",
      "iteration: 278, current_objective: 0.025902668759811617\n",
      "iteration: 279, current_objective: 0.025842289578553314\n",
      "iteration: 280, current_objective: 0.02578191039729501\n",
      "iteration: 281, current_objective: 0.02572153121603671\n",
      "iteration: 282, current_objective: 0.02566115203477841\n",
      "iteration: 283, current_objective: 0.025600772853520106\n",
      "iteration: 284, current_objective: 0.025540393672261803\n",
      "iteration: 285, current_objective: 0.0254800144910035\n",
      "iteration: 286, current_objective: 0.0254196353097452\n",
      "iteration: 287, current_objective: 0.0253592561284869\n",
      "iteration: 288, current_objective: 0.025298876947228596\n",
      "iteration: 289, current_objective: 0.025238497765970293\n",
      "iteration: 290, current_objective: 0.02517811858471199\n",
      "iteration: 291, current_objective: 0.02511773940345369\n",
      "iteration: 292, current_objective: 0.025057360222195388\n",
      "iteration: 293, current_objective: 0.024996981040937085\n",
      "iteration: 294, current_objective: 0.024936601859678782\n",
      "iteration: 295, current_objective: 0.02487622267842048\n",
      "iteration: 296, current_objective: 0.02481584349716218\n",
      "iteration: 297, current_objective: 0.024755464315903877\n",
      "iteration: 298, current_objective: 0.024695085134645575\n",
      "iteration: 299, current_objective: 0.02463470595338727\n",
      "iteration: 300, current_objective: 0.02457432677212897\n",
      "iteration: 301, current_objective: 0.024513947590870666\n",
      "iteration: 302, current_objective: 0.024453568409612367\n",
      "iteration: 303, current_objective: 0.024393189228354064\n",
      "iteration: 304, current_objective: 0.02433281004709576\n",
      "iteration: 305, current_objective: 0.02427243086583746\n",
      "iteration: 306, current_objective: 0.024212051684579156\n",
      "iteration: 307, current_objective: 0.024151672503320856\n",
      "iteration: 308, current_objective: 0.024091293322062553\n",
      "iteration: 309, current_objective: 0.02403091414080425\n",
      "iteration: 310, current_objective: 0.023970534959545948\n",
      "iteration: 311, current_objective: 0.023910155778287645\n",
      "iteration: 312, current_objective: 0.023849776597029346\n",
      "iteration: 313, current_objective: 0.023789397415771043\n",
      "iteration: 314, current_objective: 0.02372901823451274\n",
      "iteration: 315, current_objective: 0.023668639053254437\n",
      "iteration: 316, current_objective: 0.023608259871996135\n",
      "iteration: 317, current_objective: 0.023547880690737835\n",
      "iteration: 318, current_objective: 0.023487501509479532\n",
      "iteration: 319, current_objective: 0.02342712232822123\n",
      "iteration: 320, current_objective: 0.023366743146962927\n",
      "iteration: 321, current_objective: 0.023306363965704624\n",
      "iteration: 322, current_objective: 0.02324598478444632\n",
      "iteration: 323, current_objective: 0.023185605603188022\n",
      "iteration: 324, current_objective: 0.02312522642192972\n",
      "iteration: 325, current_objective: 0.023064847240671416\n",
      "iteration: 326, current_objective: 0.023004468059413113\n",
      "iteration: 327, current_objective: 0.02294408887815481\n",
      "iteration: 328, current_objective: 0.02288370969689651\n",
      "iteration: 329, current_objective: 0.02282333051563821\n",
      "iteration: 330, current_objective: 0.022762951334379906\n",
      "iteration: 331, current_objective: 0.022702572153121603\n",
      "iteration: 332, current_objective: 0.0226421929718633\n",
      "iteration: 333, current_objective: 0.022581813790605\n",
      "iteration: 334, current_objective: 0.022521434609346698\n",
      "iteration: 335, current_objective: 0.022461055428088395\n",
      "iteration: 336, current_objective: 0.022400676246830092\n",
      "iteration: 337, current_objective: 0.02234029706557179\n",
      "iteration: 338, current_objective: 0.02227991788431349\n",
      "iteration: 339, current_objective: 0.022219538703055187\n",
      "iteration: 340, current_objective: 0.022159159521796885\n",
      "iteration: 341, current_objective: 0.022098780340538582\n",
      "iteration: 342, current_objective: 0.02203840115928028\n",
      "iteration: 343, current_objective: 0.02197802197802198\n",
      "iteration: 344, current_objective: 0.021917642796763677\n",
      "iteration: 345, current_objective: 0.021857263615505374\n",
      "iteration: 346, current_objective: 0.02179688443424707\n",
      "iteration: 347, current_objective: 0.02173650525298877\n",
      "iteration: 348, current_objective: 0.021676126071730466\n",
      "iteration: 349, current_objective: 0.021615746890472166\n",
      "iteration: 350, current_objective: 0.021555367709213864\n",
      "iteration: 351, current_objective: 0.02149498852795556\n",
      "iteration: 352, current_objective: 0.021434609346697258\n",
      "iteration: 353, current_objective: 0.021374230165438955\n",
      "iteration: 354, current_objective: 0.021313850984180656\n",
      "iteration: 355, current_objective: 0.021253471802922353\n",
      "iteration: 356, current_objective: 0.02119309262166405\n",
      "iteration: 357, current_objective: 0.021132713440405747\n",
      "iteration: 358, current_objective: 0.021072334259147445\n",
      "iteration: 359, current_objective: 0.021011955077889145\n",
      "iteration: 360, current_objective: 0.020951575896630843\n",
      "iteration: 361, current_objective: 0.02089119671537254\n",
      "iteration: 362, current_objective: 0.020830817534114237\n",
      "iteration: 363, current_objective: 0.020770438352855934\n",
      "iteration: 364, current_objective: 0.020710059171597635\n",
      "iteration: 365, current_objective: 0.020649679990339332\n",
      "iteration: 366, current_objective: 0.02058930080908103\n",
      "iteration: 367, current_objective: 0.020528921627822726\n",
      "iteration: 368, current_objective: 0.020468542446564424\n",
      "iteration: 369, current_objective: 0.02040816326530612\n",
      "iteration: 370, current_objective: 0.02034778408404782\n",
      "iteration: 371, current_objective: 0.02028740490278952\n",
      "iteration: 372, current_objective: 0.020227025721531216\n",
      "iteration: 373, current_objective: 0.020166646540272913\n",
      "iteration: 374, current_objective: 0.02010626735901461\n",
      "iteration: 375, current_objective: 0.02004588817775631\n",
      "iteration: 376, current_objective: 0.019985508996498008\n"
     ]
    }
   ],
   "source": [
    "# lig_adjacency_matrix, lig_weighted_adjacency_matrix = sat_to_lig_adjacency_matrix(sat_instance, num_vars)\n",
    "max_len = 8\n",
    "clique_candidates = get_clique_candidates(graph_prime, max_len)\n",
    "num_clauses = 377\n",
    "num_vars = 91\n",
    "print(weighted_graph_prime.shape)\n",
    "current_clique_idxs, current_objective = greedy_clique_edge_cover(weighted_graph_prime, clique_candidates, num_clauses, num_vars)\n",
    "current_cliques = [clique_candidates[idx] for idx in current_clique_idxs]\n",
    "current_sat = cliques_to_sat(current_cliques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, -50, -51, -68, -91],\n",
       " [19, 23, -50, 51, -68, -91],\n",
       " [-13, -49, -50, -81, 90],\n",
       " [-17, 22, 71, -72],\n",
       " [-50, 51, -51, -68, -91],\n",
       " [-35, 59, -61, 65],\n",
       " [41, 53, -70, -72],\n",
       " [-61, -63, 78, 82, 84],\n",
       " [3, -13, -50, -91],\n",
       " [-3, 51, -51, 67],\n",
       " [-8, 9, 45, 46, -60],\n",
       " [-14, -15, -31, 57, 79],\n",
       " [19, 38, 39, 41, 54],\n",
       " [-32, 35, -35, -59],\n",
       " [19, 23, -50, 51, 68, -91],\n",
       " [-13, -81, -85, 90],\n",
       " [-19, 65, 74, -74],\n",
       " [22, 53, 71, -72],\n",
       " [-1, -32, 35, 59],\n",
       " [-35, -61, 65, 82],\n",
       " [41, 54, -70, -72],\n",
       " [45, -46, -47, -60],\n",
       " [-8, -42, 83, -83, -87],\n",
       " [-22, -38, 53, -53, 71],\n",
       " [52, 78, -78, 84, -84],\n",
       " [2, 3, 56],\n",
       " [3, 4, 23],\n",
       " [-3, -19, -51, 67],\n",
       " [-16, -50, -51, -89],\n",
       " [-18, -19, -23, -41],\n",
       " [10, 20, -21, 22, -69],\n",
       " [3, 56, -62],\n",
       " [3, -51, -68, -91],\n",
       " [7, -42, 45, 46],\n",
       " [-19, -27, -33, -34],\n",
       " [-38, -55, -72, 83],\n",
       " [-5, 37, 52, -63, 82],\n",
       " [-16, 49, 67],\n",
       " [-2, -59, 61, -65],\n",
       " [-3, 66, 67, 91],\n",
       " [-4, -61, -63, 84],\n",
       " [8, 9, -47, -48],\n",
       " [-8, 46, 47, -64],\n",
       " [-15, 57, -73, 79],\n",
       " [19, 23, 39, 41],\n",
       " [-35, 65, -74, 82],\n",
       " [-46, -47, -60, -64],\n",
       " [21, 38, 69, 70, 72],\n",
       " [-26, -27, -28, 31, 57],\n",
       " [1, 2, 56],\n",
       " [1, 41, -70],\n",
       " [2, 24, -35],\n",
       " [3, 4, 50],\n",
       " [4, 16, 19],\n",
       " [6, -37, -82],\n",
       " [7, 64, -83],\n",
       " [-19, -51, -85],\n",
       " [22, 54, -72],\n",
       " [38, -72, -77],\n",
       " [-69, -70, -72],\n",
       " [3, -13, -50, -67],\n",
       " [-4, -61, 65, -78],\n",
       " [10, 11, 12, -28],\n",
       " [10, 58, -73, -79],\n",
       " [-15, -30, -31, -76],\n",
       " [-18, -19, -38, -41],\n",
       " [-33, 65, 74, 78],\n",
       " [-36, -61, -63, 78],\n",
       " [42, -43, 46, 83],\n",
       " [1, 56, -62],\n",
       " [-1, 24, 59],\n",
       " [3, 4, 89],\n",
       " [4, 51, 68],\n",
       " [9, 46, 64],\n",
       " [9, 47, -47],\n",
       " [-16, 49, -89],\n",
       " [20, -38, 41],\n",
       " [20, -41, -70],\n",
       " [-36, -37, -82],\n",
       " [-38, -72, -77],\n",
       " [-66, 68, 90],\n",
       " [-70, 71, -72],\n",
       " [-1, -22, 70, -88],\n",
       " [-1, -32, 35, -59],\n",
       " [-5, -6, 61, 62],\n",
       " [-8, -43, -77, 88],\n",
       " [-11, 13, 76, 80],\n",
       " [12, -12, -31, -79],\n",
       " [-12, 78, -78, 79],\n",
       " [-13, -49, -81, 90],\n",
       " [19, 23, 32, -80],\n",
       " [-19, -23, -28, -34],\n",
       " [-20, -21, 77, -88],\n",
       " [-23, -37, 65, 82],\n",
       " [25, -65, 74, -74],\n",
       " [50, 66, -89, -90],\n",
       " [1, 59],\n",
       " [1, -10, -70],\n",
       " [1, 42, 60],\n",
       " [-1, -70, 77],\n",
       " [2, 6, -61],\n",
       " [-2, -35, -59],\n",
       " [3, -13, -91],\n",
       " [3, 23, 56],\n",
       " [-15, 28, 29],\n",
       " [-16, -23, 67],\n",
       " [17, 44, -77],\n",
       " [-17, -18, -19],\n",
       " [-17, 22, -72],\n",
       " [-19, 30, -74],\n",
       " [-19, -59, -61],\n",
       " [-23, 51, -89],\n",
       " [-23, -81, 85],\n",
       " [-25, -75, -76],\n",
       " [-32, 33, 56],\n",
       " [-32, -35, 59],\n",
       " [33, -62, -74],\n",
       " [35, 37, 61],\n",
       " [45, -45, 55],\n",
       " [49, 50, 67],\n",
       " [49, -86, 91],\n",
       " [68, -85, 90],\n",
       " [7, 8, -48, 55],\n",
       " [7, -42, 46, 47],\n",
       " [-7, 42, -46, -60],\n",
       " [10, 20, 22, 88],\n",
       " [12, 57, -78, -79],\n",
       " [13, 78, 79, -84],\n",
       " [21, -21, 70, 88],\n",
       " [40, -42, -83, -87],\n",
       " [51, -51, 67, -68],\n",
       " [2, 3],\n",
       " [3, 4],\n",
       " [3, 59],\n",
       " [3, -62],\n",
       " [4, -37],\n",
       " [5, -62],\n",
       " [5, 63],\n",
       " [-9, -40],\n",
       " [-9, 81],\n",
       " [-13, -76],\n",
       " [-19, 81],\n",
       " [20, -77],\n",
       " [23, -81],\n",
       " [24, 56],\n",
       " [28, -59],\n",
       " [-30, -75],\n",
       " [65, -82],\n",
       " [-70, 71],\n",
       " [-71, 72],\n",
       " [85, 91],\n",
       " [1, 48, 55],\n",
       " [-1, -2, 62],\n",
       " [-1, -10, -26],\n",
       " [-1, 19, 32],\n",
       " [4, 13, 49],\n",
       " [4, 23, 68],\n",
       " [-4, 52, 62],\n",
       " [5, 52, -63],\n",
       " [-6, 36, 37],\n",
       " [-6, 74, 82],\n",
       " [7, -40, -43],\n",
       " [-7, 17, 23],\n",
       " [-8, -83, -88],\n",
       " [9, 40, 45],\n",
       " [10, -10, -28],\n",
       " [10, 19, 21],\n",
       " [10, -20, 72],\n",
       " [10, -54, 77],\n",
       " [-10, 69, -88],\n",
       " [11, 34, 57],\n",
       " [-11, -14, -15],\n",
       " [-11, 31, 34],\n",
       " [12, -19, -84],\n",
       " [13, 29, -29],\n",
       " [13, -31, 57],\n",
       " [14, -23, -79],\n",
       " [-14, 65, 78],\n",
       " [-15, 29, -73],\n",
       " [-16, -51, -89],\n",
       " [17, 19, 39],\n",
       " [-17, 44, -77],\n",
       " [-17, -53, 77],\n",
       " [18, 60, 64],\n",
       " [-18, 47, -64],\n",
       " [-18, 54, -54],\n",
       " [19, -33, -65],\n",
       " [19, 38, 64],\n",
       " [-19, -33, 65],\n",
       " [-19, 74, -79],\n",
       " [21, 44, -69],\n",
       " [21, 54, -70],\n",
       " [22, -22, 54],\n",
       " [-22, 53, -54],\n",
       " [23, -30, 32],\n",
       " [23, 39, 48],\n",
       " [23, 41, 69],\n",
       " [23, -49, -91],\n",
       " [-23, 25, 26],\n",
       " [-23, -35, -63],\n",
       " [-23, -58, 80],\n",
       " [-23, -66, 86],\n",
       " [-23, -86, 89],\n",
       " [-24, -35, 62],\n",
       " [25, -31, 79],\n",
       " [25, 75, 76],\n",
       " [26, 27, -57],\n",
       " [26, -31, -58],\n",
       " [-26, -75, -76],\n",
       " [29, 58, 73],\n",
       " [30, 36, -82],\n",
       " [-30, 58, -73],\n",
       " [-34, 57, 76],\n",
       " [-36, -37, -63],\n",
       " [-38, 41, -72],\n",
       " [39, 42, 60],\n",
       " [-39, -53, 55],\n",
       " [43, -43, 87],\n",
       " [43, 55, -55],\n",
       " [-43, -45, -87],\n",
       " [-44, 69, -72],\n",
       " [49, -89, 91],\n",
       " [-49, 66, 86],\n",
       " [-50, 66, 90],\n",
       " [-54, 72, -88],\n",
       " [-66, -67, -89],\n",
       " [-66, 89, -90],\n",
       " [68, 86, -86],\n",
       " [-69, 70, -70],\n",
       " [-69, -72, -77],\n",
       " [1, 2],\n",
       " [1, 8],\n",
       " [1, -8],\n",
       " [1, -20],\n",
       " [1, 32],\n",
       " [1, 41],\n",
       " [1, -42],\n",
       " [1, 56],\n",
       " [1, -62],\n",
       " [1, -70],\n",
       " [1, 73],\n",
       " [-1, -6],\n",
       " [-1, 24],\n",
       " [-1, -46],\n",
       " [-1, -48],\n",
       " [-1, -56],\n",
       " [-1, -59],\n",
       " [-1, -70],\n",
       " [2, 24],\n",
       " [2, -35],\n",
       " [2, -37],\n",
       " [2, 61],\n",
       " [-2, -35],\n",
       " [-2, 52],\n",
       " [-2, 54],\n",
       " [-2, 74],\n",
       " [3, -34],\n",
       " [3, -51],\n",
       " [3, 56],\n",
       " [3, -68],\n",
       " [3, -91],\n",
       " [-3, 36],\n",
       " [-3, 51],\n",
       " [-3, 55],\n",
       " [-3, 68],\n",
       " [-3, -90],\n",
       " [-3, 91],\n",
       " [4, -6],\n",
       " [4, 16],\n",
       " [4, 19],\n",
       " [4, 30],\n",
       " [4, 51],\n",
       " [4, -52],\n",
       " [4, 61],\n",
       " [4, 63],\n",
       " [4, -67],\n",
       " [-4, -5],\n",
       " [-4, 30],\n",
       " [-4, -63],\n",
       " [-4, 65],\n",
       " [-4, 84],\n",
       " [-4, -84],\n",
       " [5, 37],\n",
       " [5, -52],\n",
       " [5, -78],\n",
       " [-5, -24],\n",
       " [-5, 35],\n",
       " [-5, -65],\n",
       " [-5, 74],\n",
       " [6, -37],\n",
       " [6, -61],\n",
       " [6, -82],\n",
       " [7, 45],\n",
       " [7, -53],\n",
       " [7, 64],\n",
       " [7, 83],\n",
       " [7, -83],\n",
       " [-7, -8],\n",
       " [-7, -42],\n",
       " [-7, -55],\n",
       " [-7, 87],\n",
       " [8, 9],\n",
       " [8, -41],\n",
       " [8, -46],\n",
       " [8, 60],\n",
       " [-8, -9],\n",
       " [-8, 22],\n",
       " [-8, 44],\n",
       " [-8, -45],\n",
       " [-8, 70],\n",
       " [9, 47],\n",
       " [9, 48],\n",
       " [9, -60],\n",
       " [9, 64],\n",
       " [-9, -30],\n",
       " [-9, -46],\n",
       " [-9, -60],\n",
       " [10, -27],\n",
       " [10, 34],\n",
       " [10, -52],\n",
       " [10, 71],\n",
       " [10, 84],\n",
       " [-10, -12],\n",
       " [-10, 27],\n",
       " [-10, 28],\n",
       " [-10, -34],\n",
       " [-10, 39],\n",
       " [11, 14],\n",
       " [11, 25],\n",
       " [-11, 12],\n",
       " [-11, -57],\n",
       " [-11, -90],\n",
       " [12, -26],\n",
       " [12, 84],\n",
       " [-12, -30],\n",
       " [-12, 56],\n",
       " [-12, -57],\n",
       " [-12, -58],\n",
       " [13, 31],\n",
       " [13, -50],\n",
       " [13, -68],\n",
       " [13, -79],\n",
       " [13, -90],\n",
       " [-13, -32],\n",
       " [-13, -67],\n",
       " [-13, -85],\n",
       " [-13, -91],\n",
       " [14, -15],\n",
       " [14, 31],\n",
       " [14, -57],\n",
       " [14, 73],\n",
       " [-14, 58],\n",
       " [-14, -90],\n",
       " [15, 25],\n",
       " [15, 30],\n",
       " [15, -33],\n",
       " [15, -57],\n",
       " [15, -58],\n",
       " [-15, -29],\n",
       " [-15, -30],\n",
       " [-15, -58],\n",
       " [-15, 76],\n",
       " [-15, -80],\n",
       " [16, 19],\n",
       " [-16, -49],\n",
       " [-16, 67],\n",
       " [17, -20],\n",
       " [17, 22],\n",
       " [17, -88],\n",
       " [-17, -19],\n",
       " [-17, 71],\n",
       " [18, 21],\n",
       " [18, 47],\n",
       " [18, 69],\n",
       " [18, -81],\n",
       " [18, 88],\n",
       " [-18, -20]]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_sat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "\n",
    "def preprocess_VIG(formula, VIG):\n",
    "    \"\"\"\n",
    "    Builds VIG.\n",
    "    \"\"\"\n",
    "    for cn in range(len(formula)):\n",
    "        for i in range(len(formula[cn]) - 1):\n",
    "            for j in range(len(formula[cn]))[i + 1 :]:\n",
    "                VIG.add_edge(abs(formula[cn][i]), abs(formula[cn][j]))\n",
    "\n",
    "                \n",
    "def preprocess_LIG(formula, LIG, num_vars):\n",
    "    for cn in range(len(formula)):\n",
    "        for i in range(len(formula[cn]) - 1):\n",
    "            for j in range(len(formula[cn]))[i + 1 :]:\n",
    "                if formula[cn][i] > 0:\n",
    "                    fst = formula[cn][i]\n",
    "                else:\n",
    "                    fst = abs(formula[cn][i]) + num_vars\n",
    "                if formula[cn][j] > 0:\n",
    "                    snd = formula[cn][j]\n",
    "                else:\n",
    "                    snd = abs(formula[cn][j]) + num_vars\n",
    "                LIG.add_edge(fst, snd)\n",
    "\n",
    "\n",
    "def preprocess_VCG(formula, VCG, num_vars):\n",
    "    \"\"\"\n",
    "    Builds VCG\n",
    "    \"\"\"\n",
    "    for cn in range(len(formula)):\n",
    "        for var in formula[cn]:\n",
    "            VCG.add_edge(abs(var), cn + num_vars + 1)\n",
    "\n",
    "\n",
    "def preprocess_LCG(formula, LCG, num_vars):\n",
    "    \"\"\"\n",
    "    Builds LCG\n",
    "    \"\"\"\n",
    "    for cn in range(len(formula)):\n",
    "        for var in formula[cn]:\n",
    "            if var > 0:\n",
    "                LCG.add_edge(abs(var), cn + num_vars + 1)\n",
    "            else:\n",
    "                LCG.add_edge(abs(var) + num_vars, cn + num_vars + 1)\n",
    "\n",
    "\n",
    "def eval_solution(sat, num_vars):\n",
    "    \n",
    "    num_clauses == len(sat)\n",
    "    \n",
    "    VIG = nx.Graph()\n",
    "    VIG.add_nodes_from(range(num_vars + 1)[1:])\n",
    "\n",
    "    LIG = nx.Graph()\n",
    "    LIG.add_nodes_from(range(num_vars * 2 + 1)[1:])\n",
    "\n",
    "    VCG = nx.Graph()\n",
    "    VCG.add_nodes_from(range(num_vars + num_clauses + 1)[1:])\n",
    "\n",
    "    LCG = nx.Graph()\n",
    "    VCG.add_nodes_from(range(2 * num_vars + num_clauses + 1)[1:])\n",
    "    \n",
    "    preprocess_VIG(sat, VIG)  # Build a VIG\n",
    "    preprocess_LIG(sat, LIG, num_vars)  # Build a LIG\n",
    "    preprocess_VCG(sat, VCG, num_vars)  # Build a VCG\n",
    "    preprocess_LCG(sat, LCG, num_vars)  # Build a LCG\n",
    "    \n",
    "    clust_VIG = nx.average_clustering(VIG)\n",
    "    clust_LIG = nx.average_clustering(LIG)\n",
    "    \n",
    "    part_VIG = community.best_partition(VIG)\n",
    "    mod_VIG = community.modularity(part_VIG, VIG)\n",
    "\n",
    "    part_LIG = community.best_partition(LIG)\n",
    "    mod_LIG = community.modularity(part_LIG, LIG)  # Modularity of VCG\n",
    "\n",
    "    part_VCG = community.best_partition(VCG)\n",
    "    mod_VCG = community.modularity(part_VCG, VCG)  # Modularity of VCG\n",
    "\n",
    "    part_LCG = community.best_partition(LCG)\n",
    "    mod_LCG = community.modularity(part_LCG, LCG)  # Modularity of LCG\n",
    "\n",
    "    \n",
    "    return [clust_VIG, clust_LIG, mod_VIG, mod_LIG, mod_VCG, mod_LCG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clu. VIG: 0.4980880667858046\n",
      "clu. LIG: 0.31905779169726217\n",
      "mod. VIG: 0.5434833333333333\n",
      "mod. LIG: 0.5948430292603858\n",
      "mod. VCG: 0.6669975962099464\n",
      "mod. LCG: 0.509564359761436\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "        \"clu. VIG\",\n",
    "        \"clu. LIG\",\n",
    "        \"mod. VIG\",\n",
    "        \"mod. LIG\",\n",
    "        \"mod. VCG\",\n",
    "        \"mod. LCG\"\n",
    "]\n",
    "\n",
    "metrics = eval_solution(current_sat, num_vars)\n",
    "for feature, value in zip(features, metrics):\n",
    "    print(f'{feature}: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7df9adec5398d45472c8bf81047aa2ae699f575f599903d765e95bf4a199fe03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
