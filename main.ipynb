{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "\n",
    "def read_sat(sat_path):\n",
    "    with open(sat_path) as f:\n",
    "        sat_lines = f.readlines()\n",
    "        header = sat_lines[0]\n",
    "        header_info = header.replace(\"\\n\", \"\").split(\" \")\n",
    "        num_vars = int(header_info[-2])\n",
    "        num_clauses = int(header_info[-1])\n",
    "\n",
    "        sat = [[int(x) for x in line.replace(' 0\\n', '').split(' ')]\n",
    "               for line in sat_lines[1:]]\n",
    "\n",
    "        return sat, num_vars, num_clauses\n",
    "\n",
    "\n",
    "def sat_to_lig_adjacency_matrix(sat, num_vars):\n",
    "    get_literal_idx = lambda x: 2 * x - 2 if x > 0 else 2 * abs(x) - 1\n",
    "    lig_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "    lig_weighted_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "\n",
    "    for clause in sat:\n",
    "        pairs = it.combinations(clause, 2)\n",
    "#         print(f'clause: {clause}')\n",
    "        for pair in pairs:\n",
    "            x_idx = get_literal_idx(pair[0])\n",
    "            y_idx = get_literal_idx(pair[1])\n",
    "#             print(f'pair: {(x_idx, y_idx)}')\n",
    "            lig_adjacency_matrix[x_idx, y_idx] = 1\n",
    "            lig_adjacency_matrix[y_idx, x_idx] = 1\n",
    "            lig_weighted_adjacency_matrix[x_idx, y_idx] += 1\n",
    "            lig_weighted_adjacency_matrix[y_idx, x_idx] += 1    \n",
    "    return lig_adjacency_matrix, lig_weighted_adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super().__init__()\n",
    "        # GCN initialization\n",
    "        self.conv1 = GCNConv(node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        # self.conv1 = GATConv(node_features, 64, 5)\n",
    "        # self.conv2 = GATConv(64 * 5, 128)\n",
    "\n",
    "        # self.conv3 = GCNConv(128, 128)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = F.elu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = F.tanh(x)\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # x1^T * W * x2\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,   0,   0, ..., 181, 181, 181]), array([  2,   4,  14, ..., 135, 169, 178]))\n",
      "tensor([[  0,   0,   0,  ..., 181, 181, 181],\n",
      "        [  2,   4,  14,  ..., 135, 169, 178]])\n",
      "[6. 6. 1. ... 2. 1. 3.]\n",
      "histogram of original weight: (array([1516,  440,   80,   60,   14,   14]), array([1, 2, 3, 4, 5, 6, 7]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6. 1. ... 2. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "sat_path = './ssa2670-141.processed.cnf'\n",
    "sat_instance, num_vars, num_clauses = read_sat(sat_path)\n",
    "\n",
    "lig_adjacency_matrix, lig_weighted_adjacency_matrix = sat_to_lig_adjacency_matrix(sat_instance, num_vars)\n",
    "print(lig_adjacency_matrix.nonzero())\n",
    "\n",
    "\n",
    "# graph = nx.from_numpy_matrix(lig_adjacency_matrix)\n",
    "# edges = nx.to_edgelist(graph)\n",
    "# print(lig_adjacency_matrix.nonzero())\n",
    "\n",
    "edge_index = torch.tensor(lig_adjacency_matrix.nonzero(), dtype=torch.long)\n",
    "print(edge_index)\n",
    "edge_value = lig_weighted_adjacency_matrix[lig_adjacency_matrix.nonzero()]\n",
    "print(edge_value)\n",
    "print(f'histogram of original weight: {np.histogram(edge_value, bins=[1, 2, 3, 4, 5, 6, 7])}')\n",
    "plt.hist(edge_value, bins=[1, 2, 3, 4, 5, 6, 7])\n",
    "plt.show()\n",
    "# max_edge_value = max(edge_value)\n",
    "# norm_edge_value = edge_value/max_edge_value\n",
    "norm_edge_value = edge_value\n",
    "print(norm_edge_value)\n",
    "\n",
    "embeddings = torch.load('./embeddings.pt')\n",
    "embeddings.requires_grad = False\n",
    "# print(embeddings)\n",
    "x = embeddings\n",
    "data = Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.1641110181808472\n",
      "epoch: 1, loss: 1.4175746440887451\n",
      "epoch: 2, loss: 0.7755216956138611\n",
      "epoch: 3, loss: 2.072573184967041\n",
      "epoch: 4, loss: 0.7336053252220154\n",
      "epoch: 5, loss: 0.8084513545036316\n",
      "epoch: 6, loss: 1.0635331869125366\n",
      "epoch: 7, loss: 1.1253913640975952\n",
      "epoch: 8, loss: 1.0059610605239868\n",
      "epoch: 9, loss: 0.7969239950180054\n",
      "epoch: 10, loss: 0.686897873878479\n",
      "epoch: 11, loss: 0.8059244751930237\n",
      "epoch: 12, loss: 0.8264654874801636\n",
      "epoch: 13, loss: 0.6877186298370361\n",
      "epoch: 14, loss: 0.609920859336853\n",
      "epoch: 15, loss: 0.629022479057312\n",
      "epoch: 16, loss: 0.6754872798919678\n",
      "epoch: 17, loss: 0.6977923512458801\n",
      "epoch: 18, loss: 0.6801174879074097\n",
      "epoch: 19, loss: 0.6346316933631897\n",
      "epoch: 20, loss: 0.5933061242103577\n",
      "epoch: 21, loss: 0.5881275534629822\n",
      "epoch: 22, loss: 0.6123658418655396\n",
      "epoch: 23, loss: 0.6167895197868347\n",
      "epoch: 24, loss: 0.5849300026893616\n",
      "epoch: 25, loss: 0.5537311434745789\n",
      "epoch: 26, loss: 0.5475904941558838\n",
      "epoch: 27, loss: 0.5561403036117554\n",
      "epoch: 28, loss: 0.560667872428894\n",
      "epoch: 29, loss: 0.554793119430542\n",
      "epoch: 30, loss: 0.5452896952629089\n",
      "epoch: 31, loss: 0.5413373112678528\n",
      "epoch: 32, loss: 0.5422452092170715\n",
      "epoch: 33, loss: 0.5388342142105103\n",
      "epoch: 34, loss: 0.5282092094421387\n",
      "epoch: 35, loss: 0.5174251794815063\n",
      "epoch: 36, loss: 0.5123142600059509\n",
      "epoch: 37, loss: 0.5113287568092346\n",
      "epoch: 38, loss: 0.509600043296814\n",
      "epoch: 39, loss: 0.5051125288009644\n",
      "epoch: 40, loss: 0.5003016591072083\n",
      "epoch: 41, loss: 0.498439222574234\n",
      "epoch: 42, loss: 0.49840596318244934\n",
      "epoch: 43, loss: 0.4954877197742462\n",
      "epoch: 44, loss: 0.48835423588752747\n",
      "epoch: 45, loss: 0.4810325503349304\n",
      "epoch: 46, loss: 0.476516455411911\n",
      "epoch: 47, loss: 0.47382083535194397\n",
      "epoch: 48, loss: 0.47097209095954895\n",
      "epoch: 49, loss: 0.46775221824645996\n",
      "epoch: 50, loss: 0.4649500250816345\n",
      "epoch: 51, loss: 0.46238768100738525\n",
      "epoch: 52, loss: 0.45892953872680664\n",
      "epoch: 53, loss: 0.4544328451156616\n",
      "epoch: 54, loss: 0.45022743940353394\n",
      "epoch: 55, loss: 0.44723719358444214\n",
      "epoch: 56, loss: 0.44484028220176697\n",
      "epoch: 57, loss: 0.44212767481803894\n",
      "epoch: 58, loss: 0.4391126334667206\n",
      "epoch: 59, loss: 0.43645188212394714\n",
      "epoch: 60, loss: 0.43432602286338806\n",
      "epoch: 61, loss: 0.4321270287036896\n",
      "epoch: 62, loss: 0.42947888374328613\n",
      "epoch: 63, loss: 0.426756352186203\n",
      "epoch: 64, loss: 0.4243919253349304\n",
      "epoch: 65, loss: 0.42234310507774353\n",
      "epoch: 66, loss: 0.42037200927734375\n",
      "epoch: 67, loss: 0.4184548258781433\n",
      "epoch: 68, loss: 0.41657406091690063\n",
      "epoch: 69, loss: 0.414610892534256\n",
      "epoch: 70, loss: 0.4125683605670929\n",
      "epoch: 71, loss: 0.41055577993392944\n",
      "epoch: 72, loss: 0.4085884094238281\n",
      "epoch: 73, loss: 0.40659284591674805\n",
      "epoch: 74, loss: 0.40459534525871277\n",
      "epoch: 75, loss: 0.40269026160240173\n",
      "epoch: 76, loss: 0.4008511006832123\n",
      "epoch: 77, loss: 0.39887872338294983\n",
      "epoch: 78, loss: 0.3967428505420685\n",
      "epoch: 79, loss: 0.3946465849876404\n",
      "epoch: 80, loss: 0.3926992416381836\n",
      "epoch: 81, loss: 0.3907700777053833\n",
      "epoch: 82, loss: 0.3887663185596466\n",
      "epoch: 83, loss: 0.3867347240447998\n",
      "epoch: 84, loss: 0.3847161829471588\n",
      "epoch: 85, loss: 0.38268256187438965\n",
      "epoch: 86, loss: 0.38062751293182373\n",
      "epoch: 87, loss: 0.37859684228897095\n",
      "epoch: 88, loss: 0.376593679189682\n",
      "epoch: 89, loss: 0.37455326318740845\n",
      "epoch: 90, loss: 0.3725149929523468\n",
      "epoch: 91, loss: 0.3704945743083954\n",
      "epoch: 92, loss: 0.3684348165988922\n",
      "epoch: 93, loss: 0.36632153391838074\n",
      "epoch: 94, loss: 0.3642183542251587\n",
      "epoch: 95, loss: 0.362148255109787\n",
      "epoch: 96, loss: 0.3600552976131439\n",
      "epoch: 97, loss: 0.3579447567462921\n",
      "epoch: 98, loss: 0.3558215796947479\n",
      "epoch: 99, loss: 0.3536776900291443\n",
      "epoch: 100, loss: 0.351531982421875\n",
      "epoch: 101, loss: 0.3493969142436981\n",
      "epoch: 102, loss: 0.3472713530063629\n",
      "epoch: 103, loss: 0.3451429605484009\n",
      "epoch: 104, loss: 0.3430194854736328\n",
      "epoch: 105, loss: 0.34089112281799316\n",
      "epoch: 106, loss: 0.33877214789390564\n",
      "epoch: 107, loss: 0.3366725444793701\n",
      "epoch: 108, loss: 0.3345943093299866\n",
      "epoch: 109, loss: 0.33252811431884766\n",
      "epoch: 110, loss: 0.33047622442245483\n",
      "epoch: 111, loss: 0.3284277021884918\n",
      "epoch: 112, loss: 0.32637500762939453\n",
      "epoch: 113, loss: 0.3243280053138733\n",
      "epoch: 114, loss: 0.32230377197265625\n",
      "epoch: 115, loss: 0.32028383016586304\n",
      "epoch: 116, loss: 0.3182556629180908\n",
      "epoch: 117, loss: 0.3162093758583069\n",
      "epoch: 118, loss: 0.3141864836215973\n",
      "epoch: 119, loss: 0.3121838867664337\n",
      "epoch: 120, loss: 0.31017670035362244\n",
      "epoch: 121, loss: 0.30817389488220215\n",
      "epoch: 122, loss: 0.3061719834804535\n",
      "epoch: 123, loss: 0.3041687607765198\n",
      "epoch: 124, loss: 0.3021804094314575\n",
      "epoch: 125, loss: 0.3001956641674042\n",
      "epoch: 126, loss: 0.29821091890335083\n",
      "epoch: 127, loss: 0.2962130606174469\n",
      "epoch: 128, loss: 0.29421791434288025\n",
      "epoch: 129, loss: 0.2922118604183197\n",
      "epoch: 130, loss: 0.29021334648132324\n",
      "epoch: 131, loss: 0.28821447491645813\n",
      "epoch: 132, loss: 0.28621575236320496\n",
      "epoch: 133, loss: 0.2842148542404175\n",
      "epoch: 134, loss: 0.28221404552459717\n",
      "epoch: 135, loss: 0.2802220284938812\n",
      "epoch: 136, loss: 0.2782294452190399\n",
      "epoch: 137, loss: 0.27625545859336853\n",
      "epoch: 138, loss: 0.27428606152534485\n",
      "epoch: 139, loss: 0.2723280191421509\n",
      "epoch: 140, loss: 0.27038130164146423\n",
      "epoch: 141, loss: 0.2684404253959656\n",
      "epoch: 142, loss: 0.2664913535118103\n",
      "epoch: 143, loss: 0.26455676555633545\n",
      "epoch: 144, loss: 0.26262450218200684\n",
      "epoch: 145, loss: 0.260695219039917\n",
      "epoch: 146, loss: 0.25876089930534363\n",
      "epoch: 147, loss: 0.2568275034427643\n",
      "epoch: 148, loss: 0.25491148233413696\n",
      "epoch: 149, loss: 0.2530139982700348\n",
      "epoch: 150, loss: 0.2511301338672638\n",
      "epoch: 151, loss: 0.24925853312015533\n",
      "epoch: 152, loss: 0.24739430844783783\n",
      "epoch: 153, loss: 0.2455390840768814\n",
      "epoch: 154, loss: 0.24369125068187714\n",
      "epoch: 155, loss: 0.24184836447238922\n",
      "epoch: 156, loss: 0.240021213889122\n",
      "epoch: 157, loss: 0.23820985853672028\n",
      "epoch: 158, loss: 0.2364014834165573\n",
      "epoch: 159, loss: 0.23458901047706604\n",
      "epoch: 160, loss: 0.23277075588703156\n",
      "epoch: 161, loss: 0.23096328973770142\n",
      "epoch: 162, loss: 0.22916166484355927\n",
      "epoch: 163, loss: 0.22737322747707367\n",
      "epoch: 164, loss: 0.22558188438415527\n",
      "epoch: 165, loss: 0.2238050252199173\n",
      "epoch: 166, loss: 0.2220277339220047\n",
      "epoch: 167, loss: 0.22024931013584137\n",
      "epoch: 168, loss: 0.21847468614578247\n",
      "epoch: 169, loss: 0.21670155227184296\n",
      "epoch: 170, loss: 0.21494126319885254\n",
      "epoch: 171, loss: 0.2131980061531067\n",
      "epoch: 172, loss: 0.21146751940250397\n",
      "epoch: 173, loss: 0.20974871516227722\n",
      "epoch: 174, loss: 0.20803983509540558\n",
      "epoch: 175, loss: 0.20634250342845917\n",
      "epoch: 176, loss: 0.2046622782945633\n",
      "epoch: 177, loss: 0.2030009627342224\n",
      "epoch: 178, loss: 0.20135745406150818\n",
      "epoch: 179, loss: 0.19973547756671906\n",
      "epoch: 180, loss: 0.19814357161521912\n",
      "epoch: 181, loss: 0.19658799469470978\n",
      "epoch: 182, loss: 0.19505874812602997\n",
      "epoch: 183, loss: 0.1935620903968811\n",
      "epoch: 184, loss: 0.1920974850654602\n",
      "epoch: 185, loss: 0.19068095088005066\n",
      "epoch: 186, loss: 0.18932946026325226\n",
      "epoch: 187, loss: 0.18812748789787292\n",
      "epoch: 188, loss: 0.18712085485458374\n",
      "epoch: 189, loss: 0.18637070059776306\n",
      "epoch: 190, loss: 0.18586020171642303\n",
      "epoch: 191, loss: 0.1844552755355835\n",
      "epoch: 192, loss: 0.18291883170604706\n",
      "epoch: 193, loss: 0.18068791925907135\n",
      "epoch: 194, loss: 0.17900973558425903\n",
      "epoch: 195, loss: 0.1779453456401825\n",
      "epoch: 196, loss: 0.17732058465480804\n",
      "epoch: 197, loss: 0.17695392668247223\n",
      "epoch: 198, loss: 0.1761590987443924\n",
      "epoch: 199, loss: 0.17541354894638062\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model = GCN(50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    src, dst = edge_index\n",
    "    score = (out[src] * out[dst]).sum(dim=-1)\n",
    "    # score = torch.sigmoid(score)\n",
    "    loss = F.mse_loss(score, torch.tensor(norm_edge_value, dtype=torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch: {epoch}, loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# TBD\n",
    "# 1. product sum \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_edge_value: [6. 6. 1. ... 2. 1. 3.]\n",
      "score: tensor([4.8779, 6.0709, 0.7641,  ..., 2.1305, 2.0319, 3.2457])\n",
      "min score: 0.3705979585647583\n",
      "max score: 6.070923328399658\n"
     ]
    }
   ],
   "source": [
    "out = model(data)\n",
    "src, dst = edge_index\n",
    "score = (out[src] * out[dst]).sum(dim=-1)\n",
    "# print(min(score))\n",
    "# score = torch.sigmoid(score)\n",
    "print(f\"norm_edge_value: {norm_edge_value}\")\n",
    "print(f\"score: {score.detach()}\")\n",
    "print(f\"min score: {min(score)}\")\n",
    "print(f\"max score: {max(score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<182x182 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2124 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig_adjacency_matrix\n",
    "sparse_matrix = sparse.csr_matrix(lig_adjacency_matrix)\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  10/200 Loss: 3.92775 Edge-Overlap: 0.388 Total-Time: 0\n",
      "Step:  20/200 Loss: 3.35798 Edge-Overlap: 0.570 Total-Time: 0\n"
     ]
    }
   ],
   "source": [
    "# CELL\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import torch\n",
    "\n",
    "from cell.utils import link_prediction_performance\n",
    "from cell.cell import Cell, EdgeOverlapCriterion, LinkPredictionCriterion\n",
    "from cell.graph_statistics import compute_graph_statistics\n",
    "\n",
    "cell_model = Cell(A=sparse_matrix,\n",
    "             H=9,\n",
    "             callbacks=[EdgeOverlapCriterion(invoke_every=10, edge_overlap_limit=.5)])\n",
    "\n",
    "\n",
    "cell_model.train(steps=200,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_args={'lr': 0.1,\n",
    "                            'weight_decay': 1e-7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(array([  0,   0,   0, ..., 181, 181, 181]), array([  2,  14,  15, ..., 134, 135, 149]))\n"
     ]
    }
   ],
   "source": [
    "generated_graph = cell_model.sample_graph()\n",
    "print(generated_graph.A)\n",
    "print(generated_graph.A.nonzero())\n",
    "# compute_graph_statistics(generated_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2124])\n",
      "tensor([[ 0.1458, -0.0650, -0.2092,  ...,  0.3151,  0.2128, -0.0984],\n",
      "        [-0.0693, -0.1420, -0.3167,  ...,  0.2207,  0.0885, -0.0737],\n",
      "        [ 0.0043, -0.1312, -0.1235,  ...,  0.1458,  0.1412, -0.0531],\n",
      "        ...,\n",
      "        [-0.1151,  0.0292,  0.0620,  ..., -0.1073, -0.0698, -0.0251],\n",
      "        [-0.1774,  0.0396,  0.1497,  ..., -0.3238, -0.2087,  0.0173],\n",
      "        [-0.1513,  0.0180,  0.2388,  ...,  0.1160,  0.1926, -0.0532]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 2.4964,  0.8891, -0.7602,  ...,  2.4207,  3.3430,  0.5872],\n",
      "       grad_fn=<SumBackward1>)\n",
      "2124\n",
      "histogram of inference weight: (array([1604,  424,   84,    6,    6,    0]), array([1, 2, 3, 4, 5, 6, 7]))\n"
     ]
    }
   ],
   "source": [
    "graph_prime = generated_graph.A\n",
    "edge_index_prime = torch.tensor(graph_prime.nonzero(), dtype=torch.long)\n",
    "print(edge_index_prime.size())\n",
    "data_prime = Data(x=x, edge_index = edge_index_prime)\n",
    "out = model(data_prime)\n",
    "print(out)\n",
    "src, dst = edge_index_prime\n",
    "score = (out[src] * out[dst]).sum(dim=-1)\n",
    "print(score)\n",
    "print(len(score))\n",
    "weight = score.detach().numpy()\n",
    "weight[weight <= 1] = 1\n",
    "weight = np.rint(weight).astype(int)\n",
    "print(f'histogram of inference weight: {np.histogram(weight, bins=[1, 2, 3, 4, 5, 6, 7])}')\n",
    "# plt.hist(weight, bins=[1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7df9adec5398d45472c8bf81047aa2ae699f575f599903d765e95bf4a199fe03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
