{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 32)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "\n",
    "def read_sat(sat_path):\n",
    "    with open(sat_path) as f:\n",
    "        sat_lines = f.readlines()\n",
    "        header = sat_lines[0]\n",
    "        header_info = header.replace(\"\\n\", \"\").split(\" \")\n",
    "        num_vars = int(header_info[-2])\n",
    "        num_clauses = int(header_info[-1])\n",
    "\n",
    "        sat = [[int(x) for x in line.replace(' 0\\n', '').split(' ')]\n",
    "               for line in sat_lines[1:]]\n",
    "\n",
    "        return sat, num_vars, num_clauses\n",
    "\n",
    "\n",
    "def sat_to_lig_adjacency_matrix(sat, num_vars):\n",
    "    get_literal_idx = lambda x: 2 * x - 2 if x > 0 else 2 * abs(x) - 1\n",
    "    lig_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "    lig_weighted_adjacency_matrix = np.zeros([2*num_vars, 2*num_vars])\n",
    "\n",
    "    for clause in sat:\n",
    "        pairs = it.combinations(clause, 2)\n",
    "#         print(f'clause: {clause}')\n",
    "        for pair in pairs:\n",
    "            x_idx = get_literal_idx(pair[0])\n",
    "            y_idx = get_literal_idx(pair[1])\n",
    "#             print(f'pair: {(x_idx, y_idx)}')\n",
    "            lig_adjacency_matrix[x_idx, y_idx] = 1\n",
    "            lig_adjacency_matrix[y_idx, x_idx] = 1\n",
    "            lig_weighted_adjacency_matrix[x_idx, y_idx] += 1\n",
    "            lig_weighted_adjacency_matrix[y_idx, x_idx] += 1    \n",
    "    return lig_adjacency_matrix, lig_weighted_adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6. 1. ... 2. 1. 3.]\n",
      "[1.         1.         0.16666667 ... 0.33333333 0.16666667 0.5       ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/6jw6ywgd3bvdbjspv5vl39nm0000gn/T/ipykernel_79117/4183645576.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_croot-4qf3nw4h/pytorch_1648016051178/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  edge_index = torch.tensor(lig_adjacency_matrix.nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "sat_path = './ssa2670-141.processed.cnf'\n",
    "sat_instance, num_vars, num_clauses = read_sat(sat_path)\n",
    "\n",
    "lig_adjacency_matrix, lig_weighted_adjacency_matrix = sat_to_lig_adjacency_matrix(sat_instance, num_vars)\n",
    "\n",
    "# graph = nx.from_numpy_matrix(lig_adjacency_matrix)\n",
    "# edges = nx.to_edgelist(graph)\n",
    "# print(lig_adjacency_matrix.nonzero())\n",
    "\n",
    "edge_index = torch.tensor(lig_adjacency_matrix.nonzero(), dtype=torch.long)\n",
    "edge_value = lig_weighted_adjacency_matrix[lig_adjacency_matrix.nonzero()]\n",
    "print(edge_value)\n",
    "max_edge_value = max(edge_value)\n",
    "norm_edge_value = edge_value/max_edge_value\n",
    "print(norm_edge_value)\n",
    "embeddings = torch.load('./embeddings.pt')\n",
    "embeddings.requires_grad = False\n",
    "# print(embeddings)\n",
    "x = embeddings\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, norm_edge_value=norm_edge_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 61.42732620239258\n",
      "epoch: 1, loss: 48.326412200927734\n",
      "epoch: 2, loss: 37.22016906738281\n",
      "epoch: 3, loss: 27.406429290771484\n",
      "epoch: 4, loss: 18.952856063842773\n",
      "epoch: 5, loss: 12.169828414916992\n",
      "epoch: 6, loss: 7.223531723022461\n",
      "epoch: 7, loss: 3.9763169288635254\n",
      "epoch: 8, loss: 2.052755117416382\n",
      "epoch: 9, loss: 1.0098166465759277\n",
      "epoch: 10, loss: 0.483527809381485\n",
      "epoch: 11, loss: 0.23297622799873352\n",
      "epoch: 12, loss: 0.12008927762508392\n",
      "epoch: 13, loss: 0.07266464084386826\n",
      "epoch: 14, loss: 0.05514013394713402\n",
      "epoch: 15, loss: 0.05061937868595123\n",
      "epoch: 16, loss: 0.05136674642562866\n",
      "epoch: 17, loss: 0.05394947528839111\n",
      "epoch: 18, loss: 0.05691593512892723\n",
      "epoch: 19, loss: 0.05969525873661041\n",
      "epoch: 20, loss: 0.06210372969508171\n",
      "epoch: 21, loss: 0.06411907821893692\n",
      "epoch: 22, loss: 0.0657782182097435\n",
      "epoch: 23, loss: 0.06713581085205078\n",
      "epoch: 24, loss: 0.0682462528347969\n",
      "epoch: 25, loss: 0.06915657967329025\n",
      "epoch: 26, loss: 0.06990573555231094\n",
      "epoch: 27, loss: 0.0705254077911377\n",
      "epoch: 28, loss: 0.07104078680276871\n",
      "epoch: 29, loss: 0.0714719370007515\n",
      "epoch: 30, loss: 0.07183466106653214\n",
      "epoch: 31, loss: 0.07214152067899704\n",
      "epoch: 32, loss: 0.07240249216556549\n",
      "epoch: 33, loss: 0.0726255550980568\n",
      "epoch: 34, loss: 0.07281716912984848\n",
      "epoch: 35, loss: 0.07298238575458527\n",
      "epoch: 36, loss: 0.07312542200088501\n",
      "epoch: 37, loss: 0.0732497125864029\n",
      "epoch: 38, loss: 0.07335805147886276\n",
      "epoch: 39, loss: 0.07345276325941086\n",
      "epoch: 40, loss: 0.07353579252958298\n",
      "epoch: 41, loss: 0.07360871881246567\n",
      "epoch: 42, loss: 0.07367291301488876\n",
      "epoch: 43, loss: 0.0737294927239418\n",
      "epoch: 44, loss: 0.07377944141626358\n",
      "epoch: 45, loss: 0.07382359355688095\n",
      "epoch: 46, loss: 0.07386264204978943\n",
      "epoch: 47, loss: 0.07389721274375916\n",
      "epoch: 48, loss: 0.07392778992652893\n",
      "epoch: 49, loss: 0.07395485043525696\n",
      "epoch: 50, loss: 0.07397880405187607\n",
      "epoch: 51, loss: 0.07399996370077133\n",
      "epoch: 52, loss: 0.07401864975690842\n",
      "epoch: 53, loss: 0.07403510063886642\n",
      "epoch: 54, loss: 0.07404958456754684\n",
      "epoch: 55, loss: 0.0740622878074646\n",
      "epoch: 56, loss: 0.07407338172197342\n",
      "epoch: 57, loss: 0.07408305257558823\n",
      "epoch: 58, loss: 0.07409141957759857\n",
      "epoch: 59, loss: 0.0740986168384552\n",
      "epoch: 60, loss: 0.07410477101802826\n",
      "epoch: 61, loss: 0.07410995662212372\n",
      "epoch: 62, loss: 0.07411429286003113\n",
      "epoch: 63, loss: 0.07411784678697586\n",
      "epoch: 64, loss: 0.07412067800760269\n",
      "epoch: 65, loss: 0.07412287592887878\n",
      "epoch: 66, loss: 0.07412448525428772\n",
      "epoch: 67, loss: 0.07412556558847427\n",
      "epoch: 68, loss: 0.07412615418434143\n",
      "epoch: 69, loss: 0.07412629574537277\n",
      "epoch: 70, loss: 0.07412604987621307\n",
      "epoch: 71, loss: 0.07412543147802353\n",
      "epoch: 72, loss: 0.07412447035312653\n",
      "epoch: 73, loss: 0.07412320375442505\n",
      "epoch: 74, loss: 0.07412165403366089\n",
      "epoch: 75, loss: 0.07411984354257584\n",
      "epoch: 76, loss: 0.07411780208349228\n",
      "epoch: 77, loss: 0.07411552965641022\n",
      "epoch: 78, loss: 0.07411305606365204\n",
      "epoch: 79, loss: 0.07411041110754013\n",
      "epoch: 80, loss: 0.0741075798869133\n",
      "epoch: 81, loss: 0.07410460710525513\n",
      "epoch: 82, loss: 0.07410146296024323\n",
      "epoch: 83, loss: 0.07409819215536118\n",
      "epoch: 84, loss: 0.07409479469060898\n",
      "epoch: 85, loss: 0.07409127056598663\n",
      "epoch: 86, loss: 0.07408764213323593\n",
      "epoch: 87, loss: 0.07408390939235687\n",
      "epoch: 88, loss: 0.07408007234334946\n",
      "epoch: 89, loss: 0.07407614588737488\n",
      "epoch: 90, loss: 0.07407213747501373\n",
      "epoch: 91, loss: 0.07406804710626602\n",
      "epoch: 92, loss: 0.07406386733055115\n",
      "epoch: 93, loss: 0.0740596204996109\n",
      "epoch: 94, loss: 0.07405529916286469\n",
      "epoch: 95, loss: 0.0740509107708931\n",
      "epoch: 96, loss: 0.07404645532369614\n",
      "epoch: 97, loss: 0.0740419402718544\n",
      "epoch: 98, loss: 0.0740373507142067\n",
      "epoch: 99, loss: 0.074032723903656\n",
      "epoch: 100, loss: 0.07402802258729935\n",
      "epoch: 101, loss: 0.0740232765674591\n",
      "epoch: 102, loss: 0.07401847839355469\n",
      "epoch: 103, loss: 0.0740136206150055\n",
      "epoch: 104, loss: 0.07400871068239212\n",
      "epoch: 105, loss: 0.07400375604629517\n",
      "epoch: 106, loss: 0.07399874925613403\n",
      "epoch: 107, loss: 0.07399369031190872\n",
      "epoch: 108, loss: 0.07398859411478043\n",
      "epoch: 109, loss: 0.07398344576358795\n",
      "epoch: 110, loss: 0.0739782452583313\n",
      "epoch: 111, loss: 0.07397300750017166\n",
      "epoch: 112, loss: 0.07396771758794785\n",
      "epoch: 113, loss: 0.07396239042282104\n",
      "epoch: 114, loss: 0.07395701110363007\n",
      "epoch: 115, loss: 0.0739515945315361\n",
      "epoch: 116, loss: 0.07394613325595856\n",
      "epoch: 117, loss: 0.07394061237573624\n",
      "epoch: 118, loss: 0.07393506169319153\n",
      "epoch: 119, loss: 0.07392946630716324\n",
      "epoch: 120, loss: 0.07392382621765137\n",
      "epoch: 121, loss: 0.07391814142465591\n",
      "epoch: 122, loss: 0.07391242682933807\n",
      "epoch: 123, loss: 0.07390664517879486\n",
      "epoch: 124, loss: 0.07390083372592926\n",
      "epoch: 125, loss: 0.07389497756958008\n",
      "epoch: 126, loss: 0.07388907670974731\n",
      "epoch: 127, loss: 0.07388313114643097\n",
      "epoch: 128, loss: 0.07387713342905045\n",
      "epoch: 129, loss: 0.07387111335992813\n",
      "epoch: 130, loss: 0.07386502623558044\n",
      "epoch: 131, loss: 0.07385891675949097\n",
      "epoch: 132, loss: 0.07385274767875671\n",
      "epoch: 133, loss: 0.07384653389453888\n",
      "epoch: 134, loss: 0.07384029775857925\n",
      "epoch: 135, loss: 0.07383400201797485\n",
      "epoch: 136, loss: 0.07382766902446747\n",
      "epoch: 137, loss: 0.0738212913274765\n",
      "epoch: 138, loss: 0.07381485402584076\n",
      "epoch: 139, loss: 0.07380838692188263\n",
      "epoch: 140, loss: 0.07380187511444092\n",
      "epoch: 141, loss: 0.07379530370235443\n",
      "epoch: 142, loss: 0.07378870248794556\n",
      "epoch: 143, loss: 0.0737820416688919\n",
      "epoch: 144, loss: 0.07377534359693527\n",
      "epoch: 145, loss: 0.07376859337091446\n",
      "epoch: 146, loss: 0.07376179844141006\n",
      "epoch: 147, loss: 0.07375495880842209\n",
      "epoch: 148, loss: 0.07374807447195053\n",
      "epoch: 149, loss: 0.0737411305308342\n",
      "epoch: 150, loss: 0.07373415678739548\n",
      "epoch: 151, loss: 0.07372711598873138\n",
      "epoch: 152, loss: 0.07372003048658371\n",
      "epoch: 153, loss: 0.07371290028095245\n",
      "epoch: 154, loss: 0.07370572537183762\n",
      "epoch: 155, loss: 0.073698490858078\n",
      "epoch: 156, loss: 0.0736912190914154\n",
      "epoch: 157, loss: 0.07368388772010803\n",
      "epoch: 158, loss: 0.07367650419473648\n",
      "epoch: 159, loss: 0.07366909086704254\n",
      "epoch: 160, loss: 0.07366160303354263\n",
      "epoch: 161, loss: 0.07365407794713974\n",
      "epoch: 162, loss: 0.07364648580551147\n",
      "epoch: 163, loss: 0.07363885641098022\n",
      "epoch: 164, loss: 0.0736311823129654\n",
      "epoch: 165, loss: 0.07362344115972519\n",
      "epoch: 166, loss: 0.0736156478524208\n",
      "epoch: 167, loss: 0.07360779494047165\n",
      "epoch: 168, loss: 0.0735999047756195\n",
      "epoch: 169, loss: 0.07359194755554199\n",
      "epoch: 170, loss: 0.0735839456319809\n",
      "epoch: 171, loss: 0.07357587665319443\n",
      "epoch: 172, loss: 0.07356774806976318\n",
      "epoch: 173, loss: 0.07355958968400955\n",
      "epoch: 174, loss: 0.07355136424303055\n",
      "epoch: 175, loss: 0.07354307919740677\n",
      "epoch: 176, loss: 0.07353474944829941\n",
      "epoch: 177, loss: 0.07352636009454727\n",
      "epoch: 178, loss: 0.07351791113615036\n",
      "epoch: 179, loss: 0.07350941002368927\n",
      "epoch: 180, loss: 0.0735008493065834\n",
      "epoch: 181, loss: 0.07349223643541336\n",
      "epoch: 182, loss: 0.07348356395959854\n",
      "epoch: 183, loss: 0.07347483187913895\n",
      "epoch: 184, loss: 0.07346605509519577\n",
      "epoch: 185, loss: 0.07345719635486603\n",
      "epoch: 186, loss: 0.0734482854604721\n",
      "epoch: 187, loss: 0.07343932241201401\n",
      "epoch: 188, loss: 0.07343029230833054\n",
      "epoch: 189, loss: 0.07342119514942169\n",
      "epoch: 190, loss: 0.07341204583644867\n",
      "epoch: 191, loss: 0.07340283691883087\n",
      "epoch: 192, loss: 0.0733935534954071\n",
      "epoch: 193, loss: 0.07338421791791916\n",
      "epoch: 194, loss: 0.07337480783462524\n",
      "epoch: 195, loss: 0.07336533814668655\n",
      "epoch: 196, loss: 0.07335580885410309\n",
      "epoch: 197, loss: 0.07334620505571365\n",
      "epoch: 198, loss: 0.07333655655384064\n",
      "epoch: 199, loss: 0.07332682609558105\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model = GCN(50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# print(norm_edge_value)\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    src, dst = edge_index\n",
    "    score = (out[src] * out[dst]).sum(dim=-1)\n",
    "    loss = F.mse_loss(score, torch.tensor(norm_edge_value, dtype=torch.float))\n",
    "    print(f'epoch: {epoch}, loss: {loss.item()}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7df9adec5398d45472c8bf81047aa2ae699f575f599903d765e95bf4a199fe03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
